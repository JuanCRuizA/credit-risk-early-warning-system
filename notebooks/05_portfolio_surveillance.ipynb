{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Notebook 05: Agentic Portfolio Surveillance and Reporting\n",
    "\n",
    "## AI Chief of Staff for Credit Risk Department\n",
    "\n",
    "---\n",
    "\n",
    "### Objectives\n",
    "\n",
    "This notebook implements an **agentic portfolio surveillance system** using the Anthropic Claude SDK. The system acts as an \"AI Chief of Staff\" for credit risk management, providing:\n",
    "\n",
    "1. **Automated Portfolio Monitoring** - Continuous surveillance of 307,511 loan applications\n",
    "2. **Risk Flagging** - Identify borrowers with deteriorating credit profiles (PD increase >15%)\n",
    "3. **Deep Dive Analysis** - SHAP explanations + market intelligence for flagged accounts\n",
    "4. **Executive Reporting** - Basel IV compliant reports with VaR, risk migration, and recommendations\n",
    "5. **Audit Trail** - SR 11-7 compliant logging of all agent actions\n",
    "\n",
    "### Regulatory Compliance\n",
    "\n",
    "| Regulation | Compliance Measure |\n",
    "|------------|--------------------|\n",
    "| **Basel IV** | IRB capital calculations, stress testing |\n",
    "| **SR 11-7** | Model documentation, audit trail, verification |\n",
    "| **ECOA/Reg B** | No discriminatory proxies in decision rationale |\n",
    "\n",
    "### Fair Lending Notice\n",
    "\n",
    "> **CRITICAL**: This agent is programmed to NEVER use protected class variables (gender, age, race, religion, marital status) in risk assessment rationale. All decisions are based solely on credit behavior and financial metrics.\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** AI Chief of Staff Agent  \n",
    "**Model:** XGBoost PD Classifier (AUC-ROC: 0.7793)  \n",
    "**Portfolio:** 307,511 loans | 8.07% default rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Claude Agent SDK loaded and API key configured\n",
      "\n",
      "Project Root: c:\\Users\\carlo\\Documents\\4.DS\\credit-risk-early-warning-system\n",
      "Python Version: 3.11.14\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1.2 IMPORT LIBRARIES AND CONFIGURE CLAUDE AGENT SDK\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import logging\n",
    "import warnings\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Callable\n",
    "from functools import wraps, lru_cache\n",
    "\n",
    "# Async support for Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# ML & Explainability\n",
    "import joblib\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Claude Agent SDK\n",
    "try:\n",
    "    from claude_agent_sdk import query, ClaudeAgentOptions, tool, create_sdk_mcp_server\n",
    "    from claude_agent_sdk import AssistantMessage, ResultMessage, TextBlock\n",
    "    AGENT_SDK_AVAILABLE = True\n",
    "except ImportError:\n",
    "    AGENT_SDK_AVAILABLE = False\n",
    "    print(\"WARNING: claude-agent-sdk not installed. Run: pip install claude-agent-sdk\")\n",
    "\n",
    "# Database\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Utilities\n",
    "from dotenv import load_dotenv\n",
    "from tabulate import tabulate\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "# Rich output\n",
    "try:\n",
    "    from rich.console import Console\n",
    "    from rich.table import Table\n",
    "    from rich.panel import Panel\n",
    "    RICH_AVAILABLE = True\n",
    "    console = Console()\n",
    "except ImportError:\n",
    "    RICH_AVAILABLE = False\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_RAW = PROJECT_ROOT / 'data' / 'raw'\n",
    "DATA_PROCESSED = PROJECT_ROOT / 'data' / 'processed'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "REPORTS_DIR = PROJECT_ROOT / 'reports'\n",
    "\n",
    "# Verify API key availability\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "if AGENT_SDK_AVAILABLE and ANTHROPIC_API_KEY:\n",
    "    print(\"\\u2705 Claude Agent SDK loaded and API key configured\")\n",
    "elif AGENT_SDK_AVAILABLE:\n",
    "    print(\"\\u26a0\\ufe0f Claude Agent SDK loaded but ANTHROPIC_API_KEY not set in .env file\")\n",
    "else:\n",
    "    print(\"\\u26a0\\ufe0f Claude Agent SDK not available. Install: pip install claude-agent-sdk\")\n",
    "\n",
    "print(f\"\\nProject Root: {PROJECT_ROOT}\")\n",
    "print(f\"Python Version: {sys.version.split()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_artifacts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model artifacts...\n",
      "==================================================\n",
      "✅ XGBoost model loaded: xgb_credit_model.pkl\n",
      "✅ Label encoders loaded: 16 encoders\n",
      "✅ Feature names loaded: 211 features\n",
      "✅ Thresholds loaded:\n",
      "   - statistical_optimal: 0.5092\n",
      "   - business_optimal: 0.5900\n",
      "   - default: 0.5000\n",
      "✅ SHAP explainer info loaded\n",
      "\n",
      "==================================================\n",
      "Model artifacts loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1.3 LOAD MODEL ARTIFACTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Loading model artifacts...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load XGBoost model\n",
    "model_path = MODELS_DIR / 'xgb_credit_model.pkl'\n",
    "if model_path.exists():\n",
    "    model = joblib.load(model_path)\n",
    "    print(f\"\\u2705 XGBoost model loaded: {model_path.name}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
    "\n",
    "# Load label encoders\n",
    "encoders_path = MODELS_DIR / 'label_encoders.pkl'\n",
    "if encoders_path.exists():\n",
    "    label_encoders = joblib.load(encoders_path)\n",
    "    print(f\"\\u2705 Label encoders loaded: {len(label_encoders)} encoders\")\n",
    "else:\n",
    "    label_encoders = {}\n",
    "    print(\"\\u26a0\\ufe0f Label encoders not found\")\n",
    "\n",
    "# Load feature names\n",
    "features_path = MODELS_DIR / 'feature_names.pkl'\n",
    "if features_path.exists():\n",
    "    feature_names = joblib.load(features_path)\n",
    "    print(f\"\\u2705 Feature names loaded: {len(feature_names)} features\")\n",
    "else:\n",
    "    feature_names = []\n",
    "    print(\"\\u26a0\\ufe0f Feature names not found\")\n",
    "\n",
    "# Load thresholds\n",
    "thresholds_path = MODELS_DIR / 'thresholds.pkl'\n",
    "if thresholds_path.exists():\n",
    "    thresholds = joblib.load(thresholds_path)\n",
    "    print(f\"\\u2705 Thresholds loaded:\")\n",
    "    for name, value in thresholds.items():\n",
    "        print(f\"   - {name}: {value:.4f}\")\n",
    "else:\n",
    "    thresholds = {'default': 0.5, 'business_optimal': 0.59, 'statistical_optimal': 0.5092}\n",
    "    print(\"\\u26a0\\ufe0f Using default thresholds\")\n",
    "\n",
    "# Load SHAP explainer info\n",
    "shap_info_path = MODELS_DIR / 'shap_explainer_info.pkl'\n",
    "if shap_info_path.exists():\n",
    "    shap_info = joblib.load(shap_info_path)\n",
    "    print(f\"\\u2705 SHAP explainer info loaded\")\n",
    "else:\n",
    "    shap_info = {}\n",
    "    print(\"\\u26a0\\ufe0f SHAP info not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Model artifacts loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "create_database",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SQLite database for portfolio surveillance...\n",
      "============================================================\n",
      "Loading application_train.csv...\n",
      "   Loaded 307,511 applications with 122 columns\n",
      "✅ Table 'loan_applications' created\n",
      "\n",
      "Loading features_train.csv...\n",
      "   Loaded 307,511 rows with 213 engineered features\n",
      "✅ Table 'engineered_features' created\n",
      "✅ Table 'audit_trail' created\n",
      "✅ Table 'watch_list' created\n",
      "✅ Table 'risk_migration' created\n",
      "✅ Database indexes created\n",
      "\n",
      "============================================================\n",
      "Database created: c:\\Users\\carlo\\Documents\\4.DS\\credit-risk-early-warning-system\\data\\portfolio_surveillance.db\n",
      "Database size: 408.8 MB\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1.4 CREATE SQLITE DATABASE FROM CSV DATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Creating SQLite database for portfolio surveillance...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Database path\n",
    "DB_PATH = PROJECT_ROOT / 'data' / 'portfolio_surveillance.db'\n",
    "\n",
    "# Create database connection\n",
    "engine = create_engine(f'sqlite:///{DB_PATH}')\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Load application data\n",
    "app_train_path = DATA_RAW / 'application_train.csv'\n",
    "if app_train_path.exists():\n",
    "    print(f\"Loading {app_train_path.name}...\")\n",
    "    df_applications = pd.read_csv(app_train_path)\n",
    "    print(f\"   Loaded {len(df_applications):,} applications with {len(df_applications.columns)} columns\")\n",
    "    \n",
    "    # Write to database\n",
    "    df_applications.to_sql('loan_applications', conn, if_exists='replace', index=False)\n",
    "    print(f\"\\u2705 Table 'loan_applications' created\")\n",
    "else:\n",
    "    print(f\"\\u274c Application data not found: {app_train_path}\")\n",
    "    df_applications = pd.DataFrame()\n",
    "\n",
    "# Load engineered features\n",
    "features_train_path = DATA_PROCESSED / 'features_train.csv'\n",
    "if features_train_path.exists():\n",
    "    print(f\"\\nLoading {features_train_path.name}...\")\n",
    "    df_features = pd.read_csv(features_train_path)\n",
    "    print(f\"   Loaded {len(df_features):,} rows with {len(df_features.columns)} engineered features\")\n",
    "    \n",
    "    # Write to database\n",
    "    df_features.to_sql('engineered_features', conn, if_exists='replace', index=False)\n",
    "    print(f\"\\u2705 Table 'engineered_features' created\")\n",
    "else:\n",
    "    print(f\"\\u26a0\\ufe0f Engineered features not found: {features_train_path}\")\n",
    "    df_features = pd.DataFrame()\n",
    "\n",
    "# Create audit trail table\n",
    "conn.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS audit_trail (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        timestamp TEXT NOT NULL,\n",
    "        action_type TEXT NOT NULL,\n",
    "        description TEXT NOT NULL,\n",
    "        data_summary TEXT,\n",
    "        decision_rationale TEXT,\n",
    "        agent_id TEXT DEFAULT 'prudent_risk_officer',\n",
    "        session_id TEXT\n",
    "    )\n",
    "''')\n",
    "print(f\"\\u2705 Table 'audit_trail' created\")\n",
    "\n",
    "# Create watch list table\n",
    "conn.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS watch_list (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        SK_ID_CURR INTEGER NOT NULL,\n",
    "        flag_date TEXT NOT NULL,\n",
    "        flag_reason TEXT NOT NULL,\n",
    "        pd_score REAL,\n",
    "        expected_loss REAL,\n",
    "        confidence_rating TEXT,\n",
    "        recommendation TEXT,\n",
    "        status TEXT DEFAULT 'Active',\n",
    "        resolved_date TEXT,\n",
    "        resolved_by TEXT\n",
    "    )\n",
    "''')\n",
    "print(f\"\\u2705 Table 'watch_list' created\")\n",
    "\n",
    "# Create risk migration table\n",
    "conn.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS risk_migration (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        SK_ID_CURR INTEGER NOT NULL,\n",
    "        observation_date TEXT NOT NULL,\n",
    "        previous_rating TEXT,\n",
    "        current_rating TEXT,\n",
    "        pd_change REAL,\n",
    "        migration_driver TEXT\n",
    "    )\n",
    "''')\n",
    "print(f\"\\u2705 Table 'risk_migration' created\")\n",
    "\n",
    "# Create indexes\n",
    "conn.execute('CREATE INDEX IF NOT EXISTS idx_app_target ON loan_applications(TARGET)')\n",
    "conn.execute('CREATE INDEX IF NOT EXISTS idx_app_id ON loan_applications(SK_ID_CURR)')\n",
    "conn.execute('CREATE INDEX IF NOT EXISTS idx_watchlist_status ON watch_list(status)')\n",
    "conn.execute('CREATE INDEX IF NOT EXISTS idx_audit_time ON audit_trail(timestamp)')\n",
    "print(f\"\\u2705 Database indexes created\")\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Database created: {DB_PATH}\")\n",
    "print(f\"Database size: {DB_PATH.stat().st_size / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configure_logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================",
    "# 1.5 CONFIGURE AUDIT LOGGING",
    "# =============================================================================",
    "",
    "# Audit log file path",
    "AUDIT_LOG_PATH = PROJECT_ROOT / 'audit_trail.log'",
    "",
    "# Configure logging",
    "logging.basicConfig(",
    "    level=logging.INFO,",
    "    format='%(asctime)s | %(levelname)s | %(name)s | %(message)s',",
    "    datefmt='%Y-%m-%d %H:%M:%S',",
    "    handlers=[",
    "        logging.FileHandler(AUDIT_LOG_PATH, mode='a'),",
    "        logging.StreamHandler(sys.stdout)",
    "    ]",
    ")",
    "",
    "# Create specialized logger for audit trail",
    "audit_logger = logging.getLogger('audit_trail')",
    "audit_logger.setLevel(logging.INFO)",
    "",
    "# Session ID for tracking",
    "SESSION_ID = datetime.now().strftime('%Y%m%d_%H%M%S') + '_' + hashlib.md5(str(datetime.now()).encode()).hexdigest()[:8]",
    "",
    "def log_audit_event(",
    "    action_type: str,",
    "    description: str,",
    "    data_summary: dict = None,",
    "    decision_rationale: str = None",
    ") -> None:",
    "    \"\"\"",
    "    Log an audit event to both file and database.",
    "    SR 11-7 compliant logging for all agent actions.",
    "    \"\"\"",
    "    timestamp = datetime.now().isoformat()",
    "    ",
    "    # Log to file",
    "    audit_logger.info(",
    "        f\"[{action_type}] {description} | \"",
    "        f\"Data: {json.dumps(data_summary, default=str) if data_summary else 'N/A'} | \"",
    "        f\"Rationale: {decision_rationale or 'N/A'}\"",
    "    )",
    "    ",
    "    # Log to database",
    "    try:",
    "        conn.execute(",
    "            '''",
    "            INSERT INTO audit_trail (timestamp, action_type, description, data_summary, decision_rationale, session_id)",
    "            VALUES (?, ?, ?, ?, ?, ?)",
    "            ''',",
    "            (timestamp, action_type, description, ",
    "             json.dumps(data_summary, default=str) if data_summary else None,",
    "             decision_rationale, SESSION_ID)",
    "        )",
    "        conn.commit()",
    "    except Exception as e:",
    "        audit_logger.error(f\"Failed to log to database: {e}\")",
    "",
    "# Log session start",
    "log_audit_event(",
    "    action_type='SESSION_START',",
    "    description='Portfolio Surveillance Agent session initialized',",
    "    data_summary={",
    "        'session_id': SESSION_ID,",
    "        'model_loaded': model is not None,",
    "        'feature_count': len(feature_names),",
    "        'threshold_business': thresholds.get('business_optimal', 0.59)",
    "    }",
    ")",
    "",
    "print(f\"\\u2705 Audit logging configured\")",
    "print(f\"   Session ID: {SESSION_ID}\")",
    "print(f\"   Log file: {AUDIT_LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vfykqmaq1hp",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 2: CORE AGENT ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "## 2.1 Agent Design Philosophy\n",
    "\n",
    "The **Prudent Risk Officer** agent is designed with the following principles:\n",
    "\n",
    "### Hierarchical Analysis Protocol\n",
    "1. **Phase A (Integrity)**: Validate data freshness and detect distribution drift\n",
    "2. **Phase B (Flagging)**: Identify borrowers with PD increase >15% or behavioral breaches\n",
    "3. **Phase C (Deep Dive)**: Search market intelligence + run SHAP explanations + stress tests\n",
    "4. **Phase D (Synthesis)**: Compile Watch List with confidence ratings\n",
    "\n",
    "### Fair Lending Compliance (ECOA/Regulation B)\n",
    "The agent is **explicitly prohibited** from using protected class variables in decision rationale:\n",
    "- ❌ CODE_GENDER (gender)\n",
    "- ❌ AGE_YEARS / DAYS_BIRTH (age)\n",
    "- ❌ NAME_FAMILY_STATUS (marital status)\n",
    "- ❌ Any ethnicity, religion, or national origin proxies\n",
    "\n",
    "### Tool Architecture\n",
    "| Tool | Purpose | Constraints |\n",
    "|------|---------|-------------|\n",
    "| `query_borrower_database` | SQL queries on loan data | SELECT only, must log purpose |\n",
    "| `search_financial_news` | Market intelligence | Informs but doesn't override model |\n",
    "| `execute_risk_analysis` | PD predictions, SHAP, stress tests | Deterministic, reproducible |\n",
    "| `generate_report_section` | Executive report generation | Basel IV compliant format |\n",
    "| `log_audit_event` | SR 11-7 audit trail | Mandatory for all actions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w1i1n1b542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.2 SYSTEM PROMPT - \"PRUDENT RISK OFFICER\" PERSONA\n",
    "# =============================================================================\n",
    "\n",
    "PRUDENT_RISK_OFFICER_SYSTEM_PROMPT = \"\"\"\n",
    "You are the AI Chief of Staff for Credit Risk Portfolio Surveillance - a \"Prudent Risk Officer\" \n",
    "agent responsible for monitoring a consumer credit portfolio of 307,511 loan applications.\n",
    "\n",
    "## YOUR IDENTITY AND MANDATE\n",
    "\n",
    "You are a seasoned credit risk professional with deep expertise in:\n",
    "- Basel IV regulatory requirements and capital adequacy\n",
    "- SR 11-7 model risk management guidelines\n",
    "- Fair lending laws (ECOA, Fair Housing Act) - you NEVER discriminate based on protected classes\n",
    "- Statistical methods for credit risk (PD, LGD, EAD, Expected Loss)\n",
    "- SHAP-based model explainability\n",
    "\n",
    "Your mandate is to provide early warning of credit deterioration while ensuring:\n",
    "1. All recommendations are explainable and auditable\n",
    "2. No decisions are influenced by discriminatory proxies (age, gender, race, religion)\n",
    "3. Mathematical consistency in all calculations\n",
    "4. Compliance with Basel IV and SR 11-7 documentation requirements\n",
    "\n",
    "## PORTFOLIO CONTEXT\n",
    "\n",
    "- Total Applications: 307,511\n",
    "- Default Rate: 8.07% (24,825 defaults)\n",
    "- Model: XGBoost classifier with AUC-ROC = 0.7793, Gini = 0.5585\n",
    "- Business Optimal Threshold: 0.59 (maximizes expected profit)\n",
    "- Statistical Optimal Threshold: 0.5092 (Youden's J)\n",
    "\n",
    "Top Risk Drivers (by SHAP importance):\n",
    "1. EXT_SOURCE_MEAN (39.0%) - External credit bureau scores\n",
    "2. ANNUITY_TO_CREDIT (12.2%) - Payment burden ratio\n",
    "3. CREDIT_TO_GOODS (10.7%) - Loan to collateral ratio\n",
    "4. EXT_SOURCE_MAX (10.0%) - Best external score\n",
    "5. AMT_ANNUITY (9.7%) - Monthly payment amount\n",
    "\n",
    "## ANALYSIS PROTOCOL\n",
    "\n",
    "Follow this hierarchical analysis pipeline for every surveillance run:\n",
    "\n",
    "### Phase A: Data Integrity Validation\n",
    "1. Check data freshness (timestamp of last update)\n",
    "2. Detect feature distribution drift using PSI (Population Stability Index)\n",
    "3. Flag any data quality issues before proceeding\n",
    "\n",
    "### Phase B: Risk Flagging Engine\n",
    "Apply these thresholds to identify borrowers requiring attention:\n",
    "- PD Increase > 15%: Flag for review\n",
    "- Behavioral indicators: >30 days past due (DPD) proxy via payment patterns\n",
    "- External score deterioration: EXT_SOURCE decrease > 0.1\n",
    "\n",
    "### Phase C: Deep Dive Analysis\n",
    "For flagged borrowers:\n",
    "1. Search relevant financial news for macroeconomic context\n",
    "2. Run SHAP explanation to identify risk drivers\n",
    "3. Perform what-if stress testing:\n",
    "   - Interest rate shock (+200 bps)\n",
    "   - Income reduction (-20%)\n",
    "   - Employment stress scenario\n",
    "\n",
    "### Phase D: Synthesis and Watch List\n",
    "1. Compile Watch List with confidence ratings (High/Medium/Low)\n",
    "2. Rank by Expected Loss (PD x LGD x EAD)\n",
    "3. Provide specific, actionable recommendations\n",
    "\n",
    "## FAIR LENDING COMPLIANCE\n",
    "\n",
    "CRITICAL: You must NEVER use or reference the following in decision rationale:\n",
    "- CODE_GENDER (gender)\n",
    "- AGE_YEARS or DAYS_BIRTH (age - except for fraud detection context)\n",
    "- Any ethnicity, religion, or national origin proxies\n",
    "- Marital status beyond credit-relevant factors\n",
    "\n",
    "When explaining risk drivers, focus on:\n",
    "- Credit behavior (payment history, utilization)\n",
    "- Financial ratios (debt-to-income, payment burden)\n",
    "- External credit scores (EXT_SOURCE variables)\n",
    "- Loan characteristics (amount, term, collateral)\n",
    "\n",
    "## OUTPUT EXPECTATIONS\n",
    "\n",
    "All your outputs must be:\n",
    "1. **Explainable**: Every conclusion must cite specific data and reasoning\n",
    "2. **Auditable**: Include timestamps and data references\n",
    "3. **Actionable**: Provide specific recommendations, not vague warnings\n",
    "4. **Compliant**: Align with Basel IV and SR 11-7 requirements\n",
    "5. **Non-discriminatory**: Free from protected class bias\n",
    "\n",
    "When uncertain, acknowledge limitations and recommend human review.\n",
    "\"\"\"\n",
    "\n",
    "print(\"System prompt defined for 'Prudent Risk Officer' persona\")\n",
    "print(f\"Prompt length: {len(PRUDENT_RISK_OFFICER_SYSTEM_PROMPT):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u8hppq6uhl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# NOTE: This cell has been COMMENTED OUT\n",
    "# The JSON schemas below were for the OLD manual Anthropic SDK approach.\n",
    "# Cell 8 now uses @tool decorators which are self-contained.\n",
    "# ====================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# 2.3 DEFINE TOOL SCHEMAS FOR CLAUDE TOOL USE\n",
    "# =============================================================================\n",
    "\n",
    "# Tool 1: SQL Query Tool\n",
    "# SQL_QUERY_TOOL = {\n",
    "#     \"name\": \"query_borrower_database\",\n",
    "#     \"description\": \"\"\"Execute SQL queries against the borrower database to retrieve loan \n",
    "#     application data, portfolio statistics, and risk metrics. The database contains \n",
    "#     307,511 loan applications with demographics, credit history, external source scores, \n",
    "#     and the TARGET variable (1=default, 0=no default).\n",
    "    \n",
    "#     IMPORTANT CONSTRAINTS:\n",
    "#     - Do NOT use demographic features (CODE_GENDER, AGE_YEARS) for decision-making\n",
    "#     - Always include SK_ID_CURR for traceability\n",
    "#     - Limit results to 1000 rows unless aggregating\n",
    "#     - Use this for data retrieval only, not for risk decisions\"\"\",\n",
    "#     \"input_schema\": {\n",
    "#         \"type\": \"object\",\n",
    "#         \"properties\": {\n",
    "#             \"query\": {\n",
    "#                 \"type\": \"string\",\n",
    "#                 \"description\": \"SQL query to execute (SELECT only, no INSERT/UPDATE/DELETE)\"\n",
    "#             },\n",
    "#             \"purpose\": {\n",
    "#                 \"type\": \"string\",\n",
    "#                 \"description\": \"Brief description of why this query is needed for audit trail\"\n",
    "#             },\n",
    "#             \"limit_override\": {\n",
    "#                 \"type\": \"integer\",\n",
    "#                 \"description\": \"Override default 1000 row limit (max 10000)\",\n",
    "#                 \"default\": 1000\n",
    "#             }\n",
    "#         },\n",
    "#         \"required\": [\"query\", \"purpose\"]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# Tool 2: News/Market Intelligence Tool\n",
    "# NEWS_SEARCH_TOOL = {\n",
    "#     \"name\": \"search_financial_news\",\n",
    "#     \"description\": \"\"\"Search financial news and market intelligence for context about \n",
    "#     economic conditions, sector risks, and macroeconomic indicators that may impact \n",
    "#     credit risk assessment. Use for deep-dive analysis on flagged borrowers or \n",
    "#     portfolio segments.\n",
    "    \n",
    "#     Sources: Financial news APIs, regulatory announcements, economic indicators.\n",
    "    \n",
    "#     IMPORTANT: News should inform but not override model-based risk assessments.\"\"\",\n",
    "#     \"input_schema\": {\n",
    "#         \"type\": \"object\",\n",
    "#         \"properties\": {\n",
    "#             \"query\": {\n",
    "#                 \"type\": \"string\",\n",
    "#                 \"description\": \"Search query for financial news (e.g., 'consumer credit defaults 2024')\"\n",
    "#             },\n",
    "#             \"category\": {\n",
    "#                 \"type\": \"string\",\n",
    "#                 \"enum\": [\"macroeconomic\", \"sector_specific\", \"regulatory\", \"market_conditions\"],\n",
    "#                 \"description\": \"Category of news to search\"\n",
    "#             },\n",
    "#             \"time_range\": {\n",
    "#                 \"type\": \"string\",\n",
    "#                 \"enum\": [\"24h\", \"7d\", \"30d\", \"90d\"],\n",
    "#                 \"description\": \"Time range for news search\",\n",
    "#                 \"default\": \"30d\"\n",
    "#             },\n",
    "#             \"max_results\": {\n",
    "#                 \"type\": \"integer\",\n",
    "#                 \"description\": \"Maximum number of news articles to return\",\n",
    "#                 \"default\": 5\n",
    "#             }\n",
    "#         },\n",
    "#         \"required\": [\"query\", \"category\"]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# Tool 3: Python Executor Tool (PD Model & SHAP)\n",
    "# PYTHON_EXECUTOR_TOOL = {\n",
    "#     \"name\": \"execute_risk_analysis\",\n",
    "#     \"description\": \"\"\"Execute Python code for risk analysis tasks including:\n",
    "#     - Running PD (Probability of Default) model predictions on borrower segments\n",
    "#     - Generating SHAP explanations for individual predictions\n",
    "#     - Performing stress testing and what-if scenarios\n",
    "#     - Calculating portfolio-level risk metrics (VaR, Expected Loss)\n",
    "    \n",
    "#     The executor has access to pre-loaded objects:\n",
    "#     - model: Trained XGBoost classifier\n",
    "#     - feature_names: List of feature column names\n",
    "#     - thresholds: Dict with statistical_optimal (0.5092) and business_optimal (0.59)\n",
    "#     - df_features: DataFrame with engineered features\n",
    "    \n",
    "#     CONSTRAINTS:\n",
    "#     - Code must be deterministic and reproducible\n",
    "#     - All calculations must be logged for audit\n",
    "#     - Cannot modify the underlying model or data\"\"\",\n",
    "#     \"input_schema\": {\n",
    "#         \"type\": \"object\",\n",
    "#         \"properties\": {\n",
    "#             \"code\": {\n",
    "#                 \"type\": \"string\",\n",
    "#                 \"description\": \"Python code to execute for risk analysis\"\n",
    "#             },\n",
    "#             \"analysis_type\": {\n",
    "#                 \"type\": \"string\",\n",
    "#                 \"enum\": [\"prediction\", \"shap_explanation\", \"stress_test\", \"portfolio_metrics\", \"drift_detection\"],\n",
    "#                 \"description\": \"Type of analysis being performed\"\n",
    "#             },\n",
    "#             \"borrower_ids\": {\n",
    "#                 \"type\": \"array\",\n",
    "#                 \"items\": {\"type\": \"integer\"},\n",
    "#                 \"description\": \"Optional list of SK_ID_CURR values to analyze\"\n",
    "#             },\n",
    "#             \"scenario_name\": {\n",
    "#                 \"type\": \"string\",\n",
    "#                 \"description\": \"Name of stress test scenario (for audit trail)\"\n",
    "#             }\n",
    "#         },\n",
    "#         \"required\": [\"code\", \"analysis_type\"]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# Tool 4: Report Generator Tool\n",
    "# REPORT_GENERATOR_TOOL = {\n",
    "#     \"name\": \"generate_report_section\",\n",
    "#     \"description\": \"\"\"Generate formatted sections of the Executive Credit Portfolio Health \n",
    "#     Report in Markdown format. Each section must include:\n",
    "#     - Clear headers and structure\n",
    "#     - Data-driven insights with specific numbers\n",
    "#     - Explainable conclusions (no black-box statements)\n",
    "#     - Compliance with fair lending (no discriminatory proxies)\n",
    "    \n",
    "#     Report sections available:\n",
    "#     - executive_summary: High-level portfolio health overview\n",
    "#     - var_summary: Value at Risk calculations and interpretation\n",
    "#     - risk_migration: Heat map of rating changes\n",
    "#     - top_exposures: Top 10 riskiest borrowers with recommendations\n",
    "#     - compliance_statement: Basel IV and fair lending compliance\"\"\",\n",
    "#     \"input_schema\": {\n",
    "#         \"type\": \"object\",\n",
    "#         \"properties\": {\n",
    "#             \"section_type\": {\n",
    "#                 \"type\": \"string\",\n",
    "#                 \"enum\": [\"executive_summary\", \"var_summary\", \"risk_migration\", \"top_exposures\", \"compliance_statement\"],\n",
    "#                 \"description\": \"Type of report section to generate\"\n",
    "#             },\n",
    "#             \"data\": {\n",
    "#                 \"type\": \"object\",\n",
    "#                 \"description\": \"Structured data to include in the section\"\n",
    "#             },\n",
    "#             \"include_recommendations\": {\n",
    "#                 \"type\": \"boolean\",\n",
    "#                 \"description\": \"Whether to include actionable recommendations\",\n",
    "#                 \"default\": True\n",
    "#             }\n",
    "#         },\n",
    "#         \"required\": [\"section_type\", \"data\"]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# Tool 5: Audit Logger Tool\n",
    "# AUDIT_LOGGER_TOOL = {\n",
    "#     \"name\": \"log_audit_event\",\n",
    "#     \"description\": \"\"\"Log an event to the audit trail for regulatory compliance \n",
    "#     (SR 11-7). All significant actions, decisions, and data accesses must be logged.\n",
    "    \n",
    "#     The audit trail captures:\n",
    "#     - Timestamp (UTC)\n",
    "#     - Action type and description\n",
    "#     - Data accessed (row counts, columns used)\n",
    "#     - Model inputs and outputs\n",
    "#     - Decision rationale\n",
    "#     - User/agent identifier\"\"\",\n",
    "#     \"input_schema\": {\n",
    "#         \"type\": \"object\",\n",
    "#         \"properties\": {\n",
    "#             \"action_type\": {\n",
    "#                 \"type\": \"string\",\n",
    "#                 \"enum\": [\"data_access\", \"model_prediction\", \"risk_flag\", \"report_generation\", \"decision\", \"verification\"],\n",
    "#                 \"description\": \"Type of action being logged\"\n",
    "#             },\n",
    "#             \"description\": {\n",
    "#                 \"type\": \"string\",\n",
    "#                 \"description\": \"Detailed description of the action\"\n",
    "#             },\n",
    "#             \"data_summary\": {\n",
    "#                 \"type\": \"object\",\n",
    "#                 \"description\": \"Summary of data involved (row counts, columns, etc.)\"\n",
    "#             },\n",
    "#             \"decision_rationale\": {\n",
    "#                 \"type\": \"string\",\n",
    "#                 \"description\": \"Explanation of why this action was taken (for decisions)\"\n",
    "#             }\n",
    "#         },\n",
    "#         \"required\": [\"action_type\", \"description\"]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# Combine all tools\n",
    "# AGENT_TOOLS = [\n",
    "#     SQL_QUERY_TOOL,\n",
    "#     NEWS_SEARCH_TOOL,\n",
    "#     PYTHON_EXECUTOR_TOOL,\n",
    "#     REPORT_GENERATOR_TOOL,\n",
    "#     AUDIT_LOGGER_TOOL\n",
    "# ]\n",
    "\n",
    "# print(f\"Defined {len(AGENT_TOOLS)} tools for Claude agent:\")\n",
    "# for tool in AGENT_TOOLS:\n",
    "#     print(f\"   - {tool['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y25ax49nxz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.4 TOOL IMPLEMENTATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "class FinancialNewsService:\n",
    "    \"\"\"\n",
    "    Financial news integration for portfolio surveillance.\n",
    "    Provides market intelligence and macroeconomic context.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None):\n",
    "        self.api_key = api_key or os.getenv('NEWS_API_KEY')\n",
    "        self.cache = {}\n",
    "        \n",
    "    def _cache_key(self, query: str, category: str, time_range: str) -> str:\n",
    "        return hashlib.md5(f\"{query}{category}{time_range}\".encode()).hexdigest()\n",
    "    \n",
    "    def search_news(\n",
    "        self, \n",
    "        query: str, \n",
    "        category: str = \"macroeconomic\",\n",
    "        time_range: str = \"30d\",\n",
    "        max_results: int = 5\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Search financial news with category filtering.\n",
    "        Falls back to mock data if API unavailable.\n",
    "        \"\"\"\n",
    "        cache_key = self._cache_key(query, category, time_range)\n",
    "        if cache_key in self.cache:\n",
    "            cached_time, cached_data = self.cache[cache_key]\n",
    "            if datetime.now() - cached_time < timedelta(hours=1):\n",
    "                return cached_data\n",
    "        \n",
    "        # Category-specific context\n",
    "        category_context = {\n",
    "            'macroeconomic': 'GDP, inflation, interest rates, unemployment trends',\n",
    "            'sector_specific': 'Consumer credit, lending industry, bank performance',\n",
    "            'regulatory': 'Basel IV, CFPB regulations, banking compliance updates',\n",
    "            'market_conditions': 'Credit spreads, liquidity conditions, market volatility'\n",
    "        }\n",
    "        \n",
    "        # Return structured mock data for demonstration\n",
    "        # In production, integrate with NewsAPI or Finnhub\n",
    "        result = {\n",
    "            'status': 'success',\n",
    "            'query': query,\n",
    "            'category': category,\n",
    "            'time_range': time_range,\n",
    "            'context': category_context.get(category, ''),\n",
    "            'articles': [\n",
    "                {\n",
    "                    'title': f'Market Analysis: {category.replace(\"_\", \" \").title()} Trends',\n",
    "                    'source': 'Financial Analysis Service',\n",
    "                    'published': datetime.now().isoformat(),\n",
    "                    'summary': f'Current {category} conditions show stability. '\n",
    "                               f'Key indicators for {query} remain within normal ranges.',\n",
    "                    'relevance_score': 0.85\n",
    "                },\n",
    "                {\n",
    "                    'title': f'Economic Indicators Update',\n",
    "                    'source': 'Economic Research Bureau',\n",
    "                    'published': (datetime.now() - timedelta(days=3)).isoformat(),\n",
    "                    'summary': 'Consumer credit conditions stable. Default rates tracking '\n",
    "                               'historical averages with slight uptick in subprime segment.',\n",
    "                    'relevance_score': 0.78\n",
    "                }\n",
    "            ],\n",
    "            'economic_indicators': {\n",
    "                'unemployment_rate': 4.2,\n",
    "                'inflation_rate': 3.1,\n",
    "                'fed_funds_rate': 5.25,\n",
    "                'consumer_confidence': 102.5,\n",
    "                'credit_card_delinquency_rate': 2.8\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.cache[cache_key] = (datetime.now(), result)\n",
    "        return result\n",
    "\n",
    "\n",
    "# Initialize news service\n",
    "news_service = FinancialNewsService()\n",
    "\n",
    "\n",
    "def execute_sql_query(query: str, purpose: str, limit_override: int = 1000) -> dict:\n",
    "    \"\"\"\n",
    "    Execute SQL query against the borrower database.\n",
    "    Includes safety checks and audit logging.\n",
    "    \"\"\"\n",
    "    # Safety check: Only allow SELECT statements\n",
    "    query_upper = query.strip().upper()\n",
    "    if not query_upper.startswith('SELECT'):\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': 'Only SELECT queries are allowed for safety',\n",
    "            'query': query\n",
    "        }\n",
    "    \n",
    "    # Safety check: Prevent dangerous operations\n",
    "    dangerous_keywords = ['DROP', 'DELETE', 'INSERT', 'UPDATE', 'ALTER', 'TRUNCATE']\n",
    "    for keyword in dangerous_keywords:\n",
    "        if keyword in query_upper:\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'error': f'Dangerous keyword detected: {keyword}',\n",
    "                'query': query\n",
    "            }\n",
    "    \n",
    "    # Apply limit\n",
    "    if 'LIMIT' not in query_upper:\n",
    "        query = f\"{query} LIMIT {min(limit_override, 10000)}\"\n",
    "    \n",
    "    try:\n",
    "        # Execute query\n",
    "        result_df = pd.read_sql_query(query, conn)\n",
    "        \n",
    "        # Log audit event\n",
    "        log_audit_event(\n",
    "            action_type='data_access',\n",
    "            description=f'SQL query executed: {purpose}',\n",
    "            data_summary={\n",
    "                'rows_returned': len(result_df),\n",
    "                'columns': list(result_df.columns),\n",
    "                'query_preview': query[:200]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'data': result_df.to_dict(orient='records'),\n",
    "            'row_count': len(result_df),\n",
    "            'columns': list(result_df.columns)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_audit_event(\n",
    "            action_type='data_access',\n",
    "            description=f'SQL query failed: {purpose}',\n",
    "            data_summary={'error': str(e), 'query': query[:200]}\n",
    "        )\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': str(e),\n",
    "            'query': query\n",
    "        }\n",
    "\n",
    "\n",
    "def execute_risk_analysis(\n",
    "    code: str, \n",
    "    analysis_type: str, \n",
    "    borrower_ids: List[int] = None,\n",
    "    scenario_name: str = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Execute Python code for risk analysis in a controlled environment.\n",
    "    \"\"\"\n",
    "    # Create execution context with pre-loaded objects and pipeline functions\n",
    "    exec_globals = {\n",
    "        'np': np,\n",
    "        'pd': pd,\n",
    "        'model': model,\n",
    "        'feature_names': feature_names,\n",
    "        'thresholds': thresholds,\n",
    "        'df_features': df_features,\n",
    "        'df_applications': df_applications,\n",
    "        'shap': shap,\n",
    "        'stats': stats,\n",
    "        'conn': conn,\n",
    "        # Pipeline functions (defined in later cells, resolved at call time)\n",
    "        'check_data_freshness': globals().get('check_data_freshness'),\n",
    "        'detect_drift': globals().get('detect_drift'),\n",
    "        'calculate_psi': globals().get('calculate_psi'),\n",
    "        'calculate_pd_scores': globals().get('calculate_pd_scores'),\n",
    "        'flag_pd_breaches': globals().get('flag_pd_breaches'),\n",
    "        'flag_behavioral_indicators': globals().get('flag_behavioral_indicators'),\n",
    "        'run_stress_test': globals().get('run_stress_test'),\n",
    "        'generate_watch_list': globals().get('generate_watch_list'),\n",
    "        'calculate_portfolio_var': globals().get('calculate_portfolio_var'),\n",
    "        'generate_risk_migration_matrix': globals().get('generate_risk_migration_matrix'),\n",
    "        'format_top_exposures': globals().get('format_top_exposures'),\n",
    "        'check_fair_lending_compliance': globals().get('check_fair_lending_compliance'),\n",
    "    }\n",
    "    exec_locals = {}\n",
    "    \n",
    "    try:\n",
    "        # Execute code\n",
    "        exec(code, exec_globals, exec_locals)\n",
    "        \n",
    "        # Get result (last assigned variable or 'result')\n",
    "        result = exec_locals.get('result', exec_locals)\n",
    "        \n",
    "        # Convert DataFrames to dict for JSON serialization\n",
    "        if isinstance(result, pd.DataFrame):\n",
    "            result = result.to_dict(orient='records')\n",
    "        elif isinstance(result, np.ndarray):\n",
    "            result = result.tolist()\n",
    "        \n",
    "        # Log audit event\n",
    "        log_audit_event(\n",
    "            action_type='model_prediction' if analysis_type == 'prediction' else 'data_access',\n",
    "            description=f'Risk analysis executed: {analysis_type}',\n",
    "            data_summary={\n",
    "                'analysis_type': analysis_type,\n",
    "                'scenario': scenario_name,\n",
    "                'borrower_count': len(borrower_ids) if borrower_ids else 'all'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'analysis_type': analysis_type,\n",
    "            'result': result\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_audit_event(\n",
    "            action_type='model_prediction',\n",
    "            description=f'Risk analysis failed: {analysis_type}',\n",
    "            data_summary={'error': str(e)}\n",
    "        )\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': str(e),\n",
    "            'analysis_type': analysis_type\n",
    "        }\n",
    "\n",
    "\n",
    "def search_financial_news(\n",
    "    query: str,\n",
    "    category: str,\n",
    "    time_range: str = \"30d\",\n",
    "    max_results: int = 5\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Search financial news for market intelligence.\n",
    "    \"\"\"\n",
    "    result = news_service.search_news(query, category, time_range, max_results)\n",
    "    \n",
    "    # Log audit event\n",
    "    log_audit_event(\n",
    "        action_type='data_access',\n",
    "        description=f'Financial news search: {query}',\n",
    "        data_summary={\n",
    "            'category': category,\n",
    "            'time_range': time_range,\n",
    "            'results_count': len(result.get('articles', []))\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_report_section(\n",
    "    section_type: str,\n",
    "    data: dict,\n",
    "    include_recommendations: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate formatted report sections in Markdown.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    if section_type == 'executive_summary':\n",
    "        content = f\"\"\"\n",
    "## Executive Credit Portfolio Health Report\n",
    "\n",
    "**Report Date:** {timestamp}\n",
    "**Portfolio:** Consumer Credit Loans\n",
    "**Prepared By:** AI Chief of Staff (Prudent Risk Officer Agent)\n",
    "\n",
    "---\n",
    "\n",
    "### Executive Summary\n",
    "\n",
    "The credit portfolio of **{data.get('total_applications', 307511):,}** loan applications has been analyzed.\n",
    "\n",
    "**Key Findings:**\n",
    "- **Overall Default Rate:** {data.get('default_rate', 8.07):.2f}%\n",
    "- **Model Performance:** AUC-ROC = {data.get('auc_roc', 0.7793):.4f}\n",
    "- **Watch List Count:** {data.get('watch_list_count', 0)} borrowers flagged for review\n",
    "- **Estimated Portfolio VaR (95%):** ${data.get('var_95', 0):,.0f}\n",
    "\n",
    "**Risk Status:** {data.get('risk_status', 'Stable')}\n",
    "\n",
    "{data.get('risk_summary', '')}\n",
    "\"\"\"\n",
    "    \n",
    "    elif section_type == 'var_summary':\n",
    "        content = f\"\"\"\n",
    "### Portfolio Value at Risk (VaR) Summary\n",
    "\n",
    "| Metric | Value | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| **Expected Loss** | ${data.get('expected_loss', 0):,.0f} | Average loss under normal conditions |\n",
    "| **VaR (95%, 1-year)** | ${data.get('var_95', 0):,.0f} | Loss not exceeded 95% of the time |\n",
    "| **VaR (99%, 1-year)** | ${data.get('var_99', 0):,.0f} | Loss not exceeded 99% of the time |\n",
    "| **Stressed VaR** | ${data.get('stressed_var', 0):,.0f} | VaR under adverse economic conditions |\n",
    "\n",
    "**Assumptions:**\n",
    "- Loss Given Default (LGD): {data.get('lgd', 60):.0f}%\n",
    "- Average Exposure at Default: ${data.get('avg_ead', 15000):,.0f}\n",
    "\"\"\"\n",
    "    \n",
    "    elif section_type == 'risk_migration':\n",
    "        content = f\"\"\"\n",
    "### Risk Migration Summary\n",
    "\n",
    "**Migration Summary:**\n",
    "- Upgrades: {data.get('upgrades', 0):,} ({data.get('upgrade_pct', 0):.1f}%)\n",
    "- Stable: {data.get('stable', 0):,} ({data.get('stable_pct', 0):.1f}%)\n",
    "- Downgrades: {data.get('downgrades', 0):,} ({data.get('downgrade_pct', 0):.1f}%)\n",
    "- New Defaults: {data.get('new_defaults', 0):,} ({data.get('default_pct', 0):.1f}%)\n",
    "\n",
    "{data.get('migration_narrative', '')}\n",
    "\"\"\"\n",
    "    \n",
    "    elif section_type == 'top_exposures':\n",
    "        exposures = data.get('exposures', [])\n",
    "        rows = \"\"\n",
    "        for i, exp in enumerate(exposures[:10], 1):\n",
    "            rows += f\"| {i} | {exp.get('id', 'N/A')} | {exp.get('pd_score', 0):.2%} | ${exp.get('expected_loss', 0):,.0f} | {exp.get('risk_drivers', 'N/A')} | {exp.get('recommendation', 'N/A')} |\\n\"\n",
    "        \n",
    "        content = f\"\"\"\n",
    "### Top 10 Riskiest Exposures\n",
    "\n",
    "| Rank | SK_ID_CURR | PD Score | Expected Loss | Risk Drivers | Recommendation |\n",
    "|------|------------|----------|---------------|--------------|----------------|\n",
    "{rows}\n",
    "\n",
    "**Aggregate Risk:**\n",
    "- Total Expected Loss (Top 10): ${data.get('top_10_el', 0):,.0f}\n",
    "- Concentration Ratio: {data.get('concentration', 0):.1f}% of portfolio EL\n",
    "\"\"\"\n",
    "    \n",
    "    elif section_type == 'compliance_statement':\n",
    "        content = f\"\"\"\n",
    "### Regulatory Compliance Statement\n",
    "\n",
    "#### Basel IV Compliance\n",
    "- [x] IRB model validation complete\n",
    "- [x] PD estimates based on through-the-cycle methodology\n",
    "- [x] Capital adequacy ratio within regulatory limits\n",
    "\n",
    "#### Fair Lending Compliance (ECOA/Regulation B)\n",
    "- [x] No protected class variables used in decision logic\n",
    "- [x] Disparate impact analysis: {data.get('disparate_impact_status', 'Passed')}\n",
    "- [x] Model explanations available for all decisions\n",
    "\n",
    "#### SR 11-7 Model Risk Management\n",
    "- [x] Model documentation complete\n",
    "- [x] Independent validation: {data.get('validation_status', 'Completed')}\n",
    "- [x] Ongoing monitoring in place\n",
    "- [x] Audit trail maintained\n",
    "\n",
    "**Certification:**\n",
    "This analysis was conducted in compliance with applicable regulations.\n",
    "\n",
    "*Generated by: Prudent Risk Officer Agent*\n",
    "*Timestamp: {timestamp}*\n",
    "\"\"\"\n",
    "    else:\n",
    "        content = f\"Unknown section type: {section_type}\"\n",
    "    \n",
    "    # Log audit event\n",
    "    log_audit_event(\n",
    "        action_type='report_generation',\n",
    "        description=f'Report section generated: {section_type}',\n",
    "        data_summary={'section_type': section_type, 'include_recommendations': include_recommendations}\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'status': 'success',\n",
    "        'section_type': section_type,\n",
    "        'content': content\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2.4b CLAUDE AGENT SDK TOOL WRAPPERS (@tool decorators)\n",
    "# =============================================================================\n",
    "\n",
    "if AGENT_SDK_AVAILABLE:\n",
    "    @tool(\n",
    "        \"query_borrower_database\",\n",
    "        (\n",
    "            \"Execute SQL queries against the borrower database to retrieve loan \"\n",
    "            \"application data, portfolio statistics, and risk metrics. The database contains \"\n",
    "            \"307,511 loan applications with demographics, credit history, external source scores, \"\n",
    "            \"and the TARGET variable (1=default, 0=no default). \"\n",
    "            \"CONSTRAINTS: Do NOT use demographic features for decision-making. \"\n",
    "            \"Always include SK_ID_CURR for traceability. Limit results to 1000 rows unless aggregating.\"\n",
    "        ),\n",
    "        {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\"type\": \"string\", \"description\": \"SQL query to execute (SELECT only)\"},\n",
    "                \"purpose\": {\"type\": \"string\", \"description\": \"Brief description for audit trail\"},\n",
    "                \"limit_override\": {\"type\": \"integer\", \"description\": \"Override default 1000 row limit (max 10000)\", \"default\": 1000}\n",
    "            },\n",
    "            \"required\": [\"query\", \"purpose\"]\n",
    "        }\n",
    "    )\n",
    "    async def query_borrower_database_tool(args: dict) -> dict:\n",
    "        result = execute_sql_query(\n",
    "            query=args.get(\"query\", \"\"),\n",
    "            purpose=args.get(\"purpose\", \"Unspecified\"),\n",
    "            limit_override=args.get(\"limit_override\", 1000)\n",
    "        )\n",
    "        return {\"content\": [{\"type\": \"text\", \"text\": json.dumps(result, default=str)}]}\n",
    "\n",
    "    @tool(\n",
    "        \"search_financial_news\",\n",
    "        (\n",
    "            \"Search financial news and market intelligence for economic conditions, \"\n",
    "            \"sector risks, and macroeconomic indicators impacting credit risk. \"\n",
    "            \"News should inform but not override model-based risk assessments.\"\n",
    "        ),\n",
    "        {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\"type\": \"string\", \"description\": \"Search query for financial news\"},\n",
    "                \"category\": {\"type\": \"string\", \"enum\": [\"macroeconomic\", \"sector_specific\", \"regulatory\", \"market_conditions\"]},\n",
    "                \"time_range\": {\"type\": \"string\", \"enum\": [\"24h\", \"7d\", \"30d\", \"90d\"], \"default\": \"30d\"},\n",
    "                \"max_results\": {\"type\": \"integer\", \"description\": \"Max articles to return\", \"default\": 5}\n",
    "            },\n",
    "            \"required\": [\"query\", \"category\"]\n",
    "        }\n",
    "    )\n",
    "    async def search_financial_news_tool(args: dict) -> dict:\n",
    "        result = search_financial_news(\n",
    "            query=args.get(\"query\", \"\"),\n",
    "            category=args.get(\"category\", \"macroeconomic\"),\n",
    "            time_range=args.get(\"time_range\", \"30d\"),\n",
    "            max_results=args.get(\"max_results\", 5)\n",
    "        )\n",
    "        return {\"content\": [{\"type\": \"text\", \"text\": json.dumps(result, default=str)}]}\n",
    "\n",
    "    @tool(\n",
    "        \"execute_risk_analysis\",\n",
    "        (\n",
    "            \"Execute Python code for risk analysis: PD model predictions, SHAP explanations, \"\n",
    "            \"stress testing, and portfolio metrics. Pre-loaded objects: model (XGBoost), \"\n",
    "            \"feature_names, thresholds (statistical_optimal=0.5092, business_optimal=0.59), \"\n",
    "            \"df_features, df_applications, and all pipeline functions (check_data_freshness, \"\n",
    "            \"calculate_pd_scores, flag_pd_breaches, flag_behavioral_indicators, run_stress_test, \"\n",
    "            \"generate_watch_list, calculate_portfolio_var, etc.). \"\n",
    "            \"Code must be deterministic and reproducible.\"\n",
    "        ),\n",
    "        {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"code\": {\"type\": \"string\", \"description\": \"Python code to execute\"},\n",
    "                \"analysis_type\": {\"type\": \"string\", \"enum\": [\"prediction\", \"shap_explanation\", \"stress_test\", \"portfolio_metrics\", \"drift_detection\"]},\n",
    "                \"borrower_ids\": {\"type\": \"array\", \"items\": {\"type\": \"integer\"}, \"description\": \"Optional SK_ID_CURR values\"},\n",
    "                \"scenario_name\": {\"type\": \"string\", \"description\": \"Stress test scenario name\"}\n",
    "            },\n",
    "            \"required\": [\"code\", \"analysis_type\"]\n",
    "        }\n",
    "    )\n",
    "    async def execute_risk_analysis_tool(args: dict) -> dict:\n",
    "        result = execute_risk_analysis(\n",
    "            code=args.get(\"code\", \"\"),\n",
    "            analysis_type=args.get(\"analysis_type\", \"prediction\"),\n",
    "            borrower_ids=args.get(\"borrower_ids\"),\n",
    "            scenario_name=args.get(\"scenario_name\")\n",
    "        )\n",
    "        return {\"content\": [{\"type\": \"text\", \"text\": json.dumps(result, default=str)}]}\n",
    "\n",
    "    @tool(\n",
    "        \"generate_report_section\",\n",
    "        (\n",
    "            \"Generate formatted Executive Credit Portfolio Health Report sections in Markdown. \"\n",
    "            \"Sections: executive_summary, var_summary, risk_migration, top_exposures, compliance_statement. \"\n",
    "            \"All sections must include data-driven insights with specific numbers and comply with fair lending.\"\n",
    "        ),\n",
    "        {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"section_type\": {\"type\": \"string\", \"enum\": [\"executive_summary\", \"var_summary\", \"risk_migration\", \"top_exposures\", \"compliance_statement\"]},\n",
    "                \"data\": {\"type\": \"object\", \"description\": \"Structured data for the section\"},\n",
    "                \"include_recommendations\": {\"type\": \"boolean\", \"default\": True}\n",
    "            },\n",
    "            \"required\": [\"section_type\", \"data\"]\n",
    "        }\n",
    "    )\n",
    "    async def generate_report_section_tool(args: dict) -> dict:\n",
    "        result = generate_report_section(\n",
    "            section_type=args.get(\"section_type\", \"executive_summary\"),\n",
    "            data=args.get(\"data\", {}),\n",
    "            include_recommendations=args.get(\"include_recommendations\", True)\n",
    "        )\n",
    "        return {\"content\": [{\"type\": \"text\", \"text\": json.dumps(result, default=str)}]}\n",
    "\n",
    "    @tool(\n",
    "        \"log_audit_event\",\n",
    "        (\n",
    "            \"Log an event to the SR 11-7 compliant audit trail. All significant actions, \"\n",
    "            \"decisions, and data accesses must be logged for regulatory compliance.\"\n",
    "        ),\n",
    "        {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"action_type\": {\"type\": \"string\", \"enum\": [\"data_access\", \"model_prediction\", \"risk_flag\", \"report_generation\", \"decision\", \"verification\"]},\n",
    "                \"description\": {\"type\": \"string\", \"description\": \"Detailed action description\"},\n",
    "                \"data_summary\": {\"type\": \"object\", \"description\": \"Summary of data involved\"},\n",
    "                \"decision_rationale\": {\"type\": \"string\", \"description\": \"Why this action was taken\"}\n",
    "            },\n",
    "            \"required\": [\"action_type\", \"description\"]\n",
    "        }\n",
    "    )\n",
    "    async def log_audit_event_tool(args: dict) -> dict:\n",
    "        log_audit_event(\n",
    "            action_type=args.get(\"action_type\", \"decision\"),\n",
    "            description=args.get(\"description\", \"\"),\n",
    "            data_summary=args.get(\"data_summary\"),\n",
    "            decision_rationale=args.get(\"decision_rationale\")\n",
    "        )\n",
    "        return {\"content\": [{\"type\": \"text\", \"text\": json.dumps({\"status\": \"success\", \"message\": \"Audit event logged\"}, default=str)}]}\n",
    "\n",
    "    # Collect tool wrapper references\n",
    "    AGENT_TOOL_FUNCTIONS = [\n",
    "        query_borrower_database_tool,\n",
    "        search_financial_news_tool,\n",
    "        execute_risk_analysis_tool,\n",
    "        generate_report_section_tool,\n",
    "        log_audit_event_tool\n",
    "    ]\n",
    "    print(f\"\\u2705 {len(AGENT_TOOL_FUNCTIONS)} Claude Agent SDK tool wrappers defined\")\n",
    "else:\n",
    "    AGENT_TOOL_FUNCTIONS = []\n",
    "    print(\"\\u26a0\\ufe0f Agent SDK tools not defined (SDK not available)\")\n",
    "\n",
    "print(\"\\nTool implementation functions defined:\")\n",
    "print(\"   - execute_sql_query()\")\n",
    "print(\"   - execute_risk_analysis()\")\n",
    "print(\"   - search_financial_news()\")\n",
    "print(\"   - generate_report_section()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0rgpe1faj7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.5 CREATE MCP SERVER FOR AGENT TOOLS\n",
    "# =============================================================================\n",
    "\n",
    "if AGENT_SDK_AVAILABLE and AGENT_TOOL_FUNCTIONS:\n",
    "    # Create MCP server that bundles all credit risk tools for the Agent SDK\n",
    "    risk_mcp_server = create_sdk_mcp_server(\n",
    "        name=\"credit_risk_tools\",\n",
    "        version=\"1.0.0\",\n",
    "        tools=AGENT_TOOL_FUNCTIONS\n",
    "    )\n",
    "    print(\"\\u2705 MCP server 'credit_risk_tools' created with tools:\")\n",
    "    for t in AGENT_TOOL_FUNCTIONS:\n",
    "        print(f\"   - {t.name}\")\n",
    "else:\n",
    "    risk_mcp_server = None\n",
    "    print(\"\\u26a0\\ufe0f MCP server not created (Agent SDK or tools not available)\")\n",
    "\n",
    "# Test connectivity with a simple SQL query\n",
    "test_result = execute_sql_query(\n",
    "    query=\"SELECT COUNT(*) as total FROM loan_applications\",\n",
    "    purpose=\"Test query to verify database connectivity\"\n",
    ")\n",
    "print(f\"\\nDatabase connectivity test:\")\n",
    "print(f\"   Result: {json.dumps(test_result, indent=2)[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l7zrp5luv9l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.6 PORTFOLIO SURVEILLANCE AGENT (Claude Agent SDK)\n",
    "# =============================================================================\n",
    "\n",
    "async def run_surveillance_agent(task: str, verbose: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Run the Portfolio Surveillance Agent using Claude Agent SDK.\n",
    "    \n",
    "    The agent autonomously executes a multi-turn analysis using the 5 registered\n",
    "    MCP tools: SQL queries, news search, risk analysis, report generation, and audit logging.\n",
    "    \n",
    "    Args:\n",
    "        task: The analysis task to perform (e.g., portfolio health check)\n",
    "        verbose: Print intermediate agent output\n",
    "        \n",
    "    Returns:\n",
    "        Final agent response text\n",
    "    \"\"\"\n",
    "    if not AGENT_SDK_AVAILABLE:\n",
    "        return \"ERROR: claude-agent-sdk not installed. Run: pip install claude-agent-sdk\"\n",
    "    if not ANTHROPIC_API_KEY:\n",
    "        return \"ERROR: ANTHROPIC_API_KEY not configured. Set it in .env file.\"\n",
    "    if risk_mcp_server is None:\n",
    "        return \"ERROR: MCP server not initialized. Check tool definitions.\"\n",
    "    \n",
    "    full_response = []\n",
    "    \n",
    "    try:\n",
    "        async for message in query(\n",
    "            prompt=task,\n",
    "            options=ClaudeAgentOptions(\n",
    "                system_prompt=PRUDENT_RISK_OFFICER_SYSTEM_PROMPT,\n",
    "                mcp_servers={\"credit_risk\": risk_mcp_server},\n",
    "                allowed_tools=[\n",
    "                    \"mcp__credit_risk_tools__query_borrower_database\",\n",
    "                    \"mcp__credit_risk_tools__search_financial_news\",\n",
    "                    \"mcp__credit_risk_tools__execute_risk_analysis\",\n",
    "                    \"mcp__credit_risk_tools__generate_report_section\",\n",
    "                    \"mcp__credit_risk_tools__log_audit_event\"\n",
    "                ],\n",
    "                max_turns=15,\n",
    "                model=\"claude-sonnet-4-20250514\"\n",
    "            )\n",
    "        ):\n",
    "            if isinstance(message, AssistantMessage):\n",
    "                for block in message.content:\n",
    "                    if isinstance(block, TextBlock):\n",
    "                        if verbose:\n",
    "                            print(block.text)\n",
    "                        full_response.append(block.text)\n",
    "            elif isinstance(message, ResultMessage):\n",
    "                if verbose:\n",
    "                    print(f\"\\n{'='*60}\")\n",
    "                    print(f\"Agent completed task\")\n",
    "                    print(f\"   Turns: {message.num_turns}\")\n",
    "                    print(f\"   Cost: ${message.total_cost_usd:.4f}\")\n",
    "                    print('='*60)\n",
    "                    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Agent execution failed: {str(e)}\"\n",
    "        if verbose:\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "        return error_msg\n",
    "    \n",
    "    return \"\\n\".join(full_response)\n",
    "\n",
    "\n",
    "def run_agent_sync(task: str, verbose: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Synchronous wrapper for running the surveillance agent in Jupyter notebooks.\n",
    "    \"\"\"\n",
    "    return asyncio.get_event_loop().run_until_complete(\n",
    "        run_surveillance_agent(task, verbose)\n",
    "    )\n",
    "\n",
    "\n",
    "# Status check\n",
    "if AGENT_SDK_AVAILABLE and ANTHROPIC_API_KEY and risk_mcp_server is not None:\n",
    "    print(\"\\u2705 Portfolio Surveillance Agent ready\")\n",
    "    print(f\"   Model: claude-sonnet-4-20250514\")\n",
    "    print(f\"   Max turns: 15\")\n",
    "    print(f\"   Tools: 5 (via MCP server 'credit_risk_tools')\")\n",
    "    print(f\"   SDK: claude-agent-sdk\")\n",
    "else:\n",
    "    missing = []\n",
    "    if not AGENT_SDK_AVAILABLE:\n",
    "        missing.append(\"claude-agent-sdk package\")\n",
    "    if not ANTHROPIC_API_KEY:\n",
    "        missing.append(\"ANTHROPIC_API_KEY\")\n",
    "    if risk_mcp_server is None:\n",
    "        missing.append(\"MCP server\")\n",
    "    print(f\"\\u26a0\\ufe0f Agent not ready. Missing: {', '.join(missing)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ofb8d3b78c",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 3: HIERARCHICAL ANALYSIS PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "## 3.1 Phase A: Data Integrity Validation\n",
    "\n",
    "Before conducting any risk analysis, we must validate:\n",
    "\n",
    "1. **Data Freshness** - Ensure data is current and not stale\n",
    "2. **Distribution Drift** - Detect shifts in feature distributions using:\n",
    "   - Population Stability Index (PSI)\n",
    "   - Kolmogorov-Smirnov Test\n",
    "\n",
    "### PSI Thresholds (Industry Standard)\n",
    "| PSI Value | Interpretation |\n",
    "|-----------|----------------|\n",
    "| < 0.10 | No significant shift |\n",
    "| 0.10 - 0.25 | Moderate shift - monitor |\n",
    "| > 0.25 | Significant shift - investigate |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "npol93292gs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.2 DATA FRESHNESS CHECKS\n",
    "# =============================================================================\n",
    "\n",
    "def check_data_freshness(df: pd.DataFrame, date_column: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Check data freshness and return diagnostic information.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'check_timestamp': datetime.now().isoformat(),\n",
    "        'total_records': len(df),\n",
    "        'columns': len(df.columns),\n",
    "        'memory_usage_mb': df.memory_usage(deep=True).sum() / (1024 * 1024),\n",
    "        'status': 'PASS'\n",
    "    }\n",
    "    \n",
    "    # Check for missing values in critical columns\n",
    "    critical_columns = ['SK_ID_CURR', 'TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']\n",
    "    available_critical = [col for col in critical_columns if col in df.columns]\n",
    "    \n",
    "    missing_rates = {}\n",
    "    for col in available_critical:\n",
    "        missing_rate = df[col].isna().mean() * 100\n",
    "        missing_rates[col] = f\"{missing_rate:.2f}%\"\n",
    "        \n",
    "    result['missing_rates'] = missing_rates\n",
    "    \n",
    "    # Check for data anomalies\n",
    "    if 'TARGET' in df.columns:\n",
    "        default_rate = df['TARGET'].mean() * 100\n",
    "        result['default_rate'] = f\"{default_rate:.2f}%\"\n",
    "        \n",
    "        # Flag if default rate is unusual (outside 5-15% range)\n",
    "        if default_rate < 2 or default_rate > 20:\n",
    "            result['status'] = 'WARNING'\n",
    "            result['warning'] = f\"Unusual default rate: {default_rate:.2f}%\"\n",
    "    \n",
    "    # Log audit event\n",
    "    log_audit_event(\n",
    "        action_type='data_access',\n",
    "        description='Data freshness check performed',\n",
    "        data_summary=result\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Run freshness check on loaded data\n",
    "if len(df_applications) > 0:\n",
    "    freshness_result = check_data_freshness(df_applications)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA FRESHNESS CHECK - Phase A\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Status: {freshness_result['status']}\")\n",
    "    print(f\"Total Records: {freshness_result['total_records']:,}\")\n",
    "    print(f\"Columns: {freshness_result['columns']}\")\n",
    "    print(f\"Memory Usage: {freshness_result['memory_usage_mb']:.2f} MB\")\n",
    "    print(f\"Default Rate: {freshness_result.get('default_rate', 'N/A')}\")\n",
    "    print(\"\\nMissing Rates (Critical Columns):\")\n",
    "    for col, rate in freshness_result.get('missing_rates', {}).items():\n",
    "        print(f\"   {col}: {rate}\")\n",
    "else:\n",
    "    print(\"No data loaded - skipping freshness check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xqwnk42l0ai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.3 DISTRIBUTION DRIFT DETECTION (PSI & KS-TEST)\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_psi(expected: np.ndarray, actual: np.ndarray, bins: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Population Stability Index (PSI).\n",
    "    \n",
    "    PSI measures the shift in distribution between two datasets.\n",
    "    Used for monitoring model input drift.\n",
    "    \"\"\"\n",
    "    # Handle missing values\n",
    "    expected = expected[~np.isnan(expected)]\n",
    "    actual = actual[~np.isnan(actual)]\n",
    "    \n",
    "    if len(expected) == 0 or len(actual) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # Create bins based on expected distribution\n",
    "    breakpoints = np.percentile(expected, np.linspace(0, 100, bins + 1))\n",
    "    breakpoints = np.unique(breakpoints)  # Remove duplicates\n",
    "    \n",
    "    if len(breakpoints) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate proportions in each bin\n",
    "    expected_counts = np.histogram(expected, bins=breakpoints)[0]\n",
    "    actual_counts = np.histogram(actual, bins=breakpoints)[0]\n",
    "    \n",
    "    # Convert to proportions with small epsilon to avoid division by zero\n",
    "    epsilon = 1e-10\n",
    "    expected_props = (expected_counts + epsilon) / (len(expected) + epsilon * len(breakpoints))\n",
    "    actual_props = (actual_counts + epsilon) / (len(actual) + epsilon * len(breakpoints))\n",
    "    \n",
    "    # Calculate PSI\n",
    "    psi = np.sum((actual_props - expected_props) * np.log(actual_props / expected_props))\n",
    "    \n",
    "    return psi\n",
    "\n",
    "\n",
    "def detect_drift(\n",
    "    reference_df: pd.DataFrame,\n",
    "    current_df: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    psi_threshold: float = 0.25\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detect distribution drift for specified features.\n",
    "    \n",
    "    Returns DataFrame with drift metrics for each feature.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for feature in features:\n",
    "        if feature not in reference_df.columns or feature not in current_df.columns:\n",
    "            continue\n",
    "            \n",
    "        ref_values = reference_df[feature].values\n",
    "        curr_values = current_df[feature].values\n",
    "        \n",
    "        # Calculate PSI\n",
    "        psi = calculate_psi(ref_values, curr_values)\n",
    "        \n",
    "        # KS Test\n",
    "        ref_clean = ref_values[~np.isnan(ref_values)]\n",
    "        curr_clean = curr_values[~np.isnan(curr_values)]\n",
    "        \n",
    "        if len(ref_clean) > 0 and len(curr_clean) > 0:\n",
    "            ks_stat, ks_pvalue = stats.ks_2samp(ref_clean, curr_clean)\n",
    "        else:\n",
    "            ks_stat, ks_pvalue = np.nan, np.nan\n",
    "        \n",
    "        # Determine drift status\n",
    "        if pd.isna(psi):\n",
    "            status = 'UNKNOWN'\n",
    "        elif psi > psi_threshold:\n",
    "            status = 'DRIFT_DETECTED'\n",
    "        elif psi > 0.10:\n",
    "            status = 'MONITOR'\n",
    "        else:\n",
    "            status = 'STABLE'\n",
    "        \n",
    "        results.append({\n",
    "            'feature': feature,\n",
    "            'psi': psi,\n",
    "            'ks_statistic': ks_stat,\n",
    "            'ks_pvalue': ks_pvalue,\n",
    "            'status': status\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Key features to monitor for drift (top SHAP importance)\n",
    "MONITORED_FEATURES = [\n",
    "    'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n",
    "    'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_INCOME_TOTAL',\n",
    "    'DAYS_BIRTH', 'DAYS_EMPLOYED'\n",
    "]\n",
    "\n",
    "# For demonstration, we'll use the same data split into two halves\n",
    "# In production, compare current data against historical baseline\n",
    "if len(df_applications) > 0:\n",
    "    n = len(df_applications)\n",
    "    reference_data = df_applications.iloc[:n//2]\n",
    "    current_data = df_applications.iloc[n//2:]\n",
    "    \n",
    "    drift_results = detect_drift(reference_data, current_data, MONITORED_FEATURES)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"DISTRIBUTION DRIFT DETECTION - Phase A\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nReference period: {len(reference_data):,} records\")\n",
    "    print(f\"Current period: {len(current_data):,} records\")\n",
    "    print(f\"\\nDrift Analysis Results:\")\n",
    "    print(tabulate(drift_results, headers='keys', tablefmt='grid', floatfmt='.4f'))\n",
    "    \n",
    "    # Log any drift detected\n",
    "    drift_detected = drift_results[drift_results['status'] == 'DRIFT_DETECTED']\n",
    "    if len(drift_detected) > 0:\n",
    "        log_audit_event(\n",
    "            action_type='risk_flag',\n",
    "            description='Distribution drift detected in monitored features',\n",
    "            data_summary={'drifted_features': drift_detected['feature'].tolist()}\n",
    "        )\n",
    "        print(f\"\\n[WARNING] Drift detected in {len(drift_detected)} features!\")\n",
    "    else:\n",
    "        print(\"\\n[OK] No significant drift detected\")\n",
    "else:\n",
    "    print(\"No data loaded - skipping drift detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7n5fpkpwf9d",
   "metadata": {},
   "source": [
    "## 3.4 Phase B: Risk Flagging Engine\n",
    "\n",
    "Phase B identifies borrowers requiring attention based on:\n",
    "\n",
    "1. **PD Threshold Breach** - PD score exceeds business threshold (0.59)\n",
    "2. **PD Increase** - PD increased by more than 15% from baseline\n",
    "3. **Behavioral Indicators** - Proxy signals for >30 DPD behavior\n",
    "4. **External Score Deterioration** - EXT_SOURCE decrease >0.1\n",
    "\n",
    "### Risk Tiers\n",
    "| Tier | PD Range | Action |\n",
    "|------|----------|--------|\n",
    "| **Green** | < 0.30 | Standard monitoring |\n",
    "| **Yellow** | 0.30 - 0.50 | Enhanced monitoring |\n",
    "| **Orange** | 0.50 - 0.70 | Active review |\n",
    "| **Red** | > 0.70 | Immediate action |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "umnr2acb97r",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.5 PD THRESHOLD BREACH DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_pd_scores(df: pd.DataFrame, model, feature_names: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate PD scores for all borrowers in the dataset.\n",
    "    \"\"\"\n",
    "    # Get features that exist in both dataframe and model\n",
    "    available_features = [f for f in feature_names if f in df.columns]\n",
    "    \n",
    "    if len(available_features) == 0:\n",
    "        print(\"Warning: No matching features found\")\n",
    "        return df\n",
    "    \n",
    "    # Prepare features\n",
    "    X = df[available_features].copy()\n",
    "    \n",
    "    # Handle missing features\n",
    "    for f in feature_names:\n",
    "        if f not in X.columns:\n",
    "            X[f] = 0  # Fill missing features with 0\n",
    "    \n",
    "    # Ensure column order matches model\n",
    "    X = X[feature_names]\n",
    "    \n",
    "    # Get predictions\n",
    "    # Convert object columns to numeric for XGBoost\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == \"object\":\n",
    "            X[col] = pd.Categorical(X[col]).codes\n",
    "    \n",
    "    pd_scores = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    result_df = df[['SK_ID_CURR']].copy() if 'SK_ID_CURR' in df.columns else df.iloc[:, :1].copy()\n",
    "    result_df['pd_score'] = pd_scores\n",
    "    \n",
    "    # Assign risk tier\n",
    "    conditions = [\n",
    "        (pd_scores < 0.30),\n",
    "        (pd_scores >= 0.30) & (pd_scores < 0.50),\n",
    "        (pd_scores >= 0.50) & (pd_scores < 0.70),\n",
    "        (pd_scores >= 0.70)\n",
    "    ]\n",
    "    tiers = ['Green', 'Yellow', 'Orange', 'Red']\n",
    "    result_df['risk_tier'] = np.select(conditions, tiers, default='Unknown')\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def flag_pd_breaches(\n",
    "    pd_df: pd.DataFrame,\n",
    "    threshold: float = 0.59,\n",
    "    increase_threshold: float = 0.15,\n",
    "    baseline_pd: pd.Series = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flag borrowers with PD threshold breaches.\n",
    "    \"\"\"\n",
    "    flagged = pd_df.copy()\n",
    "    \n",
    "    # Flag 1: Above business threshold\n",
    "    flagged['flag_above_threshold'] = flagged['pd_score'] > threshold\n",
    "    \n",
    "    # Flag 2: PD increase (if baseline available)\n",
    "    if baseline_pd is not None:\n",
    "        flagged['baseline_pd'] = baseline_pd\n",
    "        flagged['pd_change'] = flagged['pd_score'] - flagged['baseline_pd']\n",
    "        flagged['pd_change_pct'] = flagged['pd_change'] / (flagged['baseline_pd'] + 0.001)\n",
    "        flagged['flag_pd_increase'] = flagged['pd_change_pct'] > increase_threshold\n",
    "    else:\n",
    "        flagged['flag_pd_increase'] = False\n",
    "        flagged['pd_change_pct'] = 0\n",
    "    \n",
    "    # Combined flag\n",
    "    flagged['flagged'] = flagged['flag_above_threshold'] | flagged['flag_pd_increase']\n",
    "    \n",
    "    return flagged\n",
    "\n",
    "\n",
    "# Calculate PD scores for the portfolio\n",
    "if len(df_features) > 0 and model is not None:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PD THRESHOLD BREACH DETECTION - Phase B\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Calculate PD scores\n",
    "    pd_scores_df = calculate_pd_scores(df_features, model, feature_names)\n",
    "    \n",
    "    # Flag breaches (using business optimal threshold)\n",
    "    business_threshold = thresholds.get('business_optimal', 0.59)\n",
    "    flagged_df = flag_pd_breaches(pd_scores_df, threshold=business_threshold)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nThreshold: {business_threshold:.2f}\")\n",
    "    print(f\"\\nRisk Tier Distribution:\")\n",
    "    tier_counts = flagged_df['risk_tier'].value_counts()\n",
    "    for tier in ['Green', 'Yellow', 'Orange', 'Red']:\n",
    "        if tier in tier_counts.index:\n",
    "            count = tier_counts[tier]\n",
    "            pct = count / len(flagged_df) * 100\n",
    "            print(f\"   {tier}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Flagged borrowers\n",
    "    n_flagged = flagged_df['flagged'].sum()\n",
    "    print(f\"\\nFlagged Borrowers: {n_flagged:,} ({n_flagged/len(flagged_df)*100:.2f}%)\")\n",
    "    \n",
    "    # Log audit event\n",
    "    log_audit_event(\n",
    "        action_type='risk_flag',\n",
    "        description='PD threshold breach detection completed',\n",
    "        data_summary={\n",
    "            'total_borrowers': len(flagged_df),\n",
    "            'flagged_count': int(n_flagged),\n",
    "            'threshold': business_threshold,\n",
    "            'tier_distribution': tier_counts.to_dict()\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    print(\"Model or features not available - skipping PD calculation\")\n",
    "    flagged_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9n0f89ks0rj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.6 BEHAVIORAL THRESHOLD MONITORING (>30 DPD PROXY)\n",
    "# =============================================================================\n",
    "\n",
    "def flag_behavioral_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flag borrowers showing behavioral indicators of distress.\n",
    "    \n",
    "    Since we don't have actual DPD data, we use proxy indicators:\n",
    "    - High credit bureau overdue ratios\n",
    "    - Multiple recent credit inquiries\n",
    "    - High debt-to-income ratios\n",
    "    - Previous application refusals\n",
    "    \"\"\"\n",
    "    result = df[['SK_ID_CURR']].copy() if 'SK_ID_CURR' in df.columns else df.iloc[:, :1].copy()\n",
    "    \n",
    "    # Proxy 1: Bureau overdue indicators (if available)\n",
    "    if 'BUREAU_OVERDUE_RATIO' in df.columns:\n",
    "        result['flag_bureau_overdue'] = df['BUREAU_OVERDUE_RATIO'] > 0.1\n",
    "    elif 'AMT_REQ_CREDIT_BUREAU_YEAR' in df.columns:\n",
    "        # High inquiry volume as stress indicator\n",
    "        result['flag_bureau_overdue'] = df['AMT_REQ_CREDIT_BUREAU_YEAR'] > 8\n",
    "    else:\n",
    "        result['flag_bureau_overdue'] = False\n",
    "    \n",
    "    # Proxy 2: Payment burden (annuity to income ratio)\n",
    "    if 'PAYMENT_BURDEN' in df.columns:\n",
    "        result['flag_payment_stress'] = df['PAYMENT_BURDEN'] > 0.5  # >50% of income\n",
    "    elif 'AMT_ANNUITY' in df.columns and 'AMT_INCOME_TOTAL' in df.columns:\n",
    "        payment_burden = df['AMT_ANNUITY'] / (df['AMT_INCOME_TOTAL'] + 1)\n",
    "        result['flag_payment_stress'] = payment_burden > 0.5\n",
    "    else:\n",
    "        result['flag_payment_stress'] = False\n",
    "    \n",
    "    # Proxy 3: Previous refusals\n",
    "    if 'PREV_REFUSAL_RATE' in df.columns:\n",
    "        result['flag_prev_refusals'] = df['PREV_REFUSAL_RATE'] > 0.5\n",
    "    else:\n",
    "        result['flag_prev_refusals'] = False\n",
    "    \n",
    "    # Proxy 4: External score deterioration\n",
    "    if 'EXT_SOURCE_MIN' in df.columns:\n",
    "        result['flag_low_ext_score'] = df['EXT_SOURCE_MIN'] < 0.2\n",
    "    elif 'EXT_SOURCE_2' in df.columns:\n",
    "        result['flag_low_ext_score'] = df['EXT_SOURCE_2'] < 0.2\n",
    "    else:\n",
    "        result['flag_low_ext_score'] = False\n",
    "    \n",
    "    # Combined behavioral flag (any 2+ indicators)\n",
    "    flag_cols = ['flag_bureau_overdue', 'flag_payment_stress', 'flag_prev_refusals', 'flag_low_ext_score']\n",
    "    result['behavioral_flag_count'] = result[flag_cols].sum(axis=1)\n",
    "    result['flag_behavioral'] = result['behavioral_flag_count'] >= 2\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Apply behavioral flagging\n",
    "if len(df_features) > 0:\n",
    "    behavioral_flags = flag_behavioral_indicators(df_features)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"BEHAVIORAL THRESHOLD MONITORING - Phase B\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\nBehavioral Indicator Distribution:\")\n",
    "    for col in ['flag_bureau_overdue', 'flag_payment_stress', 'flag_prev_refusals', 'flag_low_ext_score']:\n",
    "        if col in behavioral_flags.columns:\n",
    "            count = behavioral_flags[col].sum()\n",
    "            pct = count / len(behavioral_flags) * 100\n",
    "            print(f\"   {col.replace('flag_', '').replace('_', ' ').title()}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    n_behavioral_flagged = behavioral_flags['flag_behavioral'].sum()\n",
    "    print(f\"\\nBorrowers with 2+ behavioral flags: {n_behavioral_flagged:,} ({n_behavioral_flagged/len(behavioral_flags)*100:.2f}%)\")\n",
    "    \n",
    "    # Merge with PD flags\n",
    "    if len(flagged_df) > 0:\n",
    "        flagged_df = flagged_df.merge(\n",
    "            behavioral_flags[['SK_ID_CURR', 'flag_behavioral', 'behavioral_flag_count']], \n",
    "            on='SK_ID_CURR', \n",
    "            how='left'\n",
    "        )\n",
    "        flagged_df['flag_behavioral'] = flagged_df['flag_behavioral'].fillna(False)\n",
    "        flagged_df['combined_flag'] = flagged_df['flagged'] | flagged_df['flag_behavioral']\n",
    "        \n",
    "        n_combined = flagged_df['combined_flag'].sum()\n",
    "        print(f\"\\nCombined Flagged (PD + Behavioral): {n_combined:,} ({n_combined/len(flagged_df)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"Features not available - skipping behavioral flagging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bu1vphmcqu",
   "metadata": {},
   "source": [
    "## 3.7 Phase C: Deep Dive Analysis\n",
    "\n",
    "For flagged borrowers, Phase C performs:\n",
    "\n",
    "1. **Market Intelligence Search** - Qualitative risk factors from news/sector data\n",
    "2. **SHAP Explanation** - Individual feature contributions to risk score\n",
    "3. **What-If Stress Testing** - Scenario analysis for interest coverage\n",
    "\n",
    "### Stress Test Scenarios\n",
    "| Scenario | Description | Parameters |\n",
    "|----------|-------------|------------|\n",
    "| **Interest Rate Shock** | Rate increase impact | +200 bps |\n",
    "| **Income Reduction** | Job loss / income cut | -20% income |\n",
    "| **Combined Stress** | Both scenarios | Rate +200bps, Income -20% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ekwznnm1emi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.8 NEWS / MARKET INTELLIGENCE INTEGRATION\n",
    "# =============================================================================\n",
    "\n",
    "def get_market_context(category: str = \"macroeconomic\") -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve market intelligence for portfolio context.\n",
    "    \"\"\"\n",
    "    # Search for relevant news\n",
    "    news_result = search_financial_news(\n",
    "        query=\"consumer credit default rates lending\",\n",
    "        category=category,\n",
    "        time_range=\"30d\",\n",
    "        max_results=3\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'category': category,\n",
    "        'economic_indicators': news_result.get('economic_indicators', {}),\n",
    "        'articles': news_result.get('articles', []),\n",
    "        'context': news_result.get('context', ''),\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "\n",
    "# Get market context for portfolio analysis\n",
    "print(\"=\" * 70)\n",
    "print(\"MARKET INTELLIGENCE - Phase C\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "market_context = get_market_context(\"macroeconomic\")\n",
    "\n",
    "print(\"\\nEconomic Indicators:\")\n",
    "for indicator, value in market_context.get('economic_indicators', {}).items():\n",
    "    print(f\"   {indicator.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(\"\\nRecent Market News:\")\n",
    "for i, article in enumerate(market_context.get('articles', [])[:3], 1):\n",
    "    print(f\"\\n   {i}. {article.get('title', 'N/A')}\")\n",
    "    print(f\"      Source: {article.get('source', 'N/A')}\")\n",
    "    print(f\"      Summary: {article.get('summary', 'N/A')[:100]}...\")\n",
    "\n",
    "# Log market context retrieval\n",
    "log_audit_event(\n",
    "    action_type='data_access',\n",
    "    description='Market intelligence retrieved for portfolio context',\n",
    "    data_summary={\n",
    "        'indicators_retrieved': list(market_context.get('economic_indicators', {}).keys()),\n",
    "        'articles_count': len(market_context.get('articles', []))\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jtti7gpv21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================",
    "# 3.9 WHAT-IF STRESS TESTING (INTEREST COVERAGE)",
    "# =============================================================================",
    "",
    "def run_stress_test(",
    "    df: pd.DataFrame,",
    "    model,",
    "    feature_names: list,",
    "    scenario: str = \"interest_rate_shock\",",
    "    income_reduction: float = 0.20,",
    "    rate_increase_bps: int = 200",
    ") -> pd.DataFrame:",
    "    \"\"\"",
    "    Run what-if stress test scenarios on the portfolio.",
    "    ",
    "    Scenarios:",
    "    - interest_rate_shock: Increase in interest rates (+200bps default)",
    "    - income_reduction: Reduction in borrower income (-20% default)",
    "    - combined_stress: Both shocks combined",
    "    \"\"\"",
    "    # Get available features",
    "    available_features = [f for f in feature_names if f in df.columns]",
    "    if len(available_features) == 0:",
    "        return pd.DataFrame()",
    "    ",
    "    # Create stressed data copy",
    "    stressed_df = df.copy()",
    "    ",
    "    if scenario in [\"interest_rate_shock\", \"combined_stress\"]:",
    "        # Simulate interest rate shock by increasing annuity (payment burden)",
    "        if 'AMT_ANNUITY' in stressed_df.columns:",
    "            # Approximate: 200bps increase on remaining term",
    "            rate_multiplier = 1 + (rate_increase_bps / 10000) * 5  # ~5 year avg term",
    "            stressed_df['AMT_ANNUITY'] = stressed_df['AMT_ANNUITY'] * rate_multiplier",
    "        ",
    "        if 'PAYMENT_BURDEN' in stressed_df.columns:",
    "            stressed_df['PAYMENT_BURDEN'] = stressed_df['PAYMENT_BURDEN'] * rate_multiplier",
    "        ",
    "        if 'ANNUITY_TO_CREDIT' in stressed_df.columns:",
    "            stressed_df['ANNUITY_TO_CREDIT'] = stressed_df['ANNUITY_TO_CREDIT'] * rate_multiplier",
    "    ",
    "    if scenario in [\"income_reduction\", \"combined_stress\"]:",
    "        # Simulate income reduction",
    "        if 'AMT_INCOME_TOTAL' in stressed_df.columns:",
    "            stressed_df['AMT_INCOME_TOTAL'] = stressed_df['AMT_INCOME_TOTAL'] * (1 - income_reduction)",
    "        ",
    "        if 'INCOME_TO_CREDIT' in stressed_df.columns:",
    "            stressed_df['INCOME_TO_CREDIT'] = stressed_df['INCOME_TO_CREDIT'] * (1 - income_reduction)",
    "        ",
    "        if 'DEBT_TO_INCOME' in stressed_df.columns:",
    "            stressed_df['DEBT_TO_INCOME'] = stressed_df['DEBT_TO_INCOME'] / (1 - income_reduction)",
    "        ",
    "        if 'PAYMENT_BURDEN' in stressed_df.columns:",
    "            stressed_df['PAYMENT_BURDEN'] = stressed_df['PAYMENT_BURDEN'] / (1 - income_reduction)",
    "    ",
    "    # Prepare features for prediction",
    "    X_stressed = stressed_df[available_features].copy()",
    "    for f in feature_names:",
    "        if f not in X_stressed.columns:",
    "            X_stressed[f] = 0",
    "    X_stressed = X_stressed[feature_names]",
    "    \n",
    "    # Convert object columns to numeric for XGBoost\n",
    "    for col in X_stressed.columns:\n",
    "        if X_stressed[col].dtype == \"object\":\n",
    "            X_stressed[col] = pd.Categorical(X_stressed[col]).codes\n",
    "    ",
    "    # Get baseline and stressed PD scores",
    "    X_baseline = df[available_features].copy()",
    "    for f in feature_names:",
    "        if f not in X_baseline.columns:",
    "            X_baseline[f] = 0",
    "    X_baseline = X_baseline[feature_names]",
    "    \n",
    "    # Convert object columns to numeric for XGBoost\n",
    "    for col in X_baseline.columns:\n",
    "        if X_baseline[col].dtype == \"object\":\n",
    "            X_baseline[col] = pd.Categorical(X_baseline[col]).codes\n",
    "    ",
    "    baseline_pd = model.predict_proba(X_baseline)[:, 1]",
    "    stressed_pd = model.predict_proba(X_stressed)[:, 1]",
    "    ",
    "    # Create result dataframe",
    "    result = df[['SK_ID_CURR']].copy() if 'SK_ID_CURR' in df.columns else df.iloc[:, :1].copy()",
    "    result['baseline_pd'] = baseline_pd",
    "    result['stressed_pd'] = stressed_pd",
    "    result['pd_impact'] = stressed_pd - baseline_pd",
    "    result['pd_impact_pct'] = result['pd_impact'] / (baseline_pd + 0.001) * 100",
    "    result['scenario'] = scenario",
    "    ",
    "    return result",
    "",
    "",
    "# Run stress tests on the portfolio",
    "if len(df_features) > 0 and model is not None:",
    "    print(\"=\" * 70)",
    "    print(\"WHAT-IF STRESS TESTING - Phase C\")",
    "    print(\"=\" * 70)",
    "    ",
    "    # Sample for efficiency (full portfolio in production)",
    "    sample_size = min(10000, len(df_features))",
    "    df_sample = df_features.sample(n=sample_size, random_state=42)",
    "    ",
    "    scenarios = [\"interest_rate_shock\", \"income_reduction\", \"combined_stress\"]",
    "    stress_results = {}",
    "    ",
    "    for scenario in scenarios:",
    "        result = run_stress_test(df_sample, model, feature_names, scenario=scenario)",
    "        stress_results[scenario] = result",
    "        ",
    "        avg_impact = result['pd_impact'].mean()",
    "        max_impact = result['pd_impact'].max()",
    "        pct_deteriorated = (result['pd_impact'] > 0).mean() * 100",
    "        ",
    "        print(f\"\\n{scenario.replace('_', ' ').title()}:\")",
    "        print(f\"   Average PD Impact: +{avg_impact:.4f} ({avg_impact*100:.2f}%)\")",
    "        print(f\"   Max PD Impact: +{max_impact:.4f}\")",
    "        print(f\"   Borrowers Deteriorated: {pct_deteriorated:.1f}%\")",
    "    ",
    "    # Log stress test results",
    "    log_audit_event(",
    "        action_type='model_prediction',",
    "        description='Portfolio stress testing completed',",
    "        data_summary={",
    "            'scenarios_tested': scenarios,",
    "            'sample_size': sample_size,",
    "            'combined_avg_impact': float(stress_results['combined_stress']['pd_impact'].mean())",
    "        }",
    "    )",
    "else:",
    "    print(\"Model or features not available - skipping stress testing\")",
    "    stress_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nyqmo5ylpc",
   "metadata": {},
   "source": [
    "## 3.10 Phase D: Synthesis and Watch List\n",
    "\n",
    "Phase D synthesizes all analysis into an actionable Watch List:\n",
    "\n",
    "1. **Aggregate Flags** - Combine PD, behavioral, and stress test flags\n",
    "2. **Confidence Rating** - Assign High/Medium/Low based on evidence strength\n",
    "3. **Expected Loss Ranking** - Prioritize by EL = PD x LGD x EAD\n",
    "4. **Recommendations** - Specific actions for each flagged borrower\n",
    "\n",
    "### Confidence Rating Criteria\n",
    "| Rating | Criteria |\n",
    "|--------|----------|\n",
    "| **High** | 3+ flag types + stressed PD > 0.70 |\n",
    "| **Medium** | 2 flag types OR stressed PD > 0.50 |\n",
    "| **Low** | 1 flag type + PD between threshold and 0.70 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ag6jpoheen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.11 WATCH LIST GENERATOR WITH CONFIDENCE RATINGS\n",
    "# =============================================================================\n",
    "\n",
    "def generate_watch_list(\n",
    "    flagged_df: pd.DataFrame,\n",
    "    stress_results: dict,\n",
    "    df_features: pd.DataFrame,\n",
    "    lgd: float = 0.60,\n",
    "    avg_ead: float = 15000\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate prioritized Watch List with confidence ratings and recommendations.\n",
    "    \"\"\"\n",
    "    if len(flagged_df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Start with flagged borrowers\n",
    "    watch_list = flagged_df[flagged_df.get('combined_flag', flagged_df.get('flagged', False)) == True].copy()\n",
    "    \n",
    "    if len(watch_list) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Add stressed PD if available\n",
    "    if 'combined_stress' in stress_results and len(stress_results['combined_stress']) > 0:\n",
    "        stress_df = stress_results['combined_stress'][['SK_ID_CURR', 'stressed_pd', 'pd_impact']]\n",
    "        watch_list = watch_list.merge(stress_df, on='SK_ID_CURR', how='left')\n",
    "    else:\n",
    "        watch_list['stressed_pd'] = watch_list['pd_score']\n",
    "        watch_list['pd_impact'] = 0\n",
    "    \n",
    "    # Add credit amount for EAD\n",
    "    if 'AMT_CREDIT' in df_features.columns:\n",
    "        credit_df = df_features[['SK_ID_CURR', 'AMT_CREDIT']].drop_duplicates()\n",
    "        watch_list = watch_list.merge(credit_df, on='SK_ID_CURR', how='left')\n",
    "        watch_list['ead'] = watch_list['AMT_CREDIT'].fillna(avg_ead)\n",
    "    else:\n",
    "        watch_list['ead'] = avg_ead\n",
    "    \n",
    "    # Calculate Expected Loss\n",
    "    watch_list['expected_loss'] = watch_list['pd_score'] * lgd * watch_list['ead']\n",
    "    watch_list['stressed_el'] = watch_list['stressed_pd'] * lgd * watch_list['ead']\n",
    "    \n",
    "    # Count flag types\n",
    "    flag_cols = [col for col in watch_list.columns if col.startswith('flag_') and col != 'flag_behavioral']\n",
    "    watch_list['flag_count'] = watch_list[flag_cols].sum(axis=1)\n",
    "    if 'flag_behavioral' in watch_list.columns:\n",
    "        watch_list['flag_count'] = watch_list['flag_count'] + watch_list['flag_behavioral'].astype(int)\n",
    "    \n",
    "    # Assign confidence rating\n",
    "    def assign_confidence(row):\n",
    "        stressed_pd = row.get('stressed_pd', row['pd_score'])\n",
    "        flag_count = row.get('flag_count', 1)\n",
    "        \n",
    "        if flag_count >= 3 and stressed_pd > 0.70:\n",
    "            return 'High'\n",
    "        elif flag_count >= 2 or stressed_pd > 0.50:\n",
    "            return 'Medium'\n",
    "        else:\n",
    "            return 'Low'\n",
    "    \n",
    "    watch_list['confidence_rating'] = watch_list.apply(assign_confidence, axis=1)\n",
    "    \n",
    "    # Generate recommendations\n",
    "    def generate_recommendation(row):\n",
    "        recommendations = []\n",
    "        \n",
    "        if row['pd_score'] > 0.70:\n",
    "            recommendations.append(\"Immediate collection action required\")\n",
    "        elif row['pd_score'] > 0.50:\n",
    "            recommendations.append(\"Enhanced monitoring with weekly review\")\n",
    "        \n",
    "        if row.get('flag_above_threshold', False):\n",
    "            recommendations.append(\"Review credit limit\")\n",
    "        \n",
    "        if row.get('pd_impact', 0) > 0.10:\n",
    "            recommendations.append(\"Stress-sensitive: assess interest rate exposure\")\n",
    "        \n",
    "        if row.get('behavioral_flag_count', 0) >= 2:\n",
    "            recommendations.append(\"Contact borrower for payment plan discussion\")\n",
    "        \n",
    "        return \"; \".join(recommendations) if recommendations else \"Standard monitoring\"\n",
    "    \n",
    "    watch_list['recommendation'] = watch_list.apply(generate_recommendation, axis=1)\n",
    "    \n",
    "    # Sort by Expected Loss (highest first)\n",
    "    watch_list = watch_list.sort_values('expected_loss', ascending=False)\n",
    "    \n",
    "    # Add flag date\n",
    "    watch_list['flag_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    return watch_list\n",
    "\n",
    "\n",
    "# Generate Watch List\n",
    "if len(flagged_df) > 0:\n",
    "    watch_list = generate_watch_list(flagged_df, stress_results, df_features)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"WATCH LIST GENERATED - Phase D\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if len(watch_list) > 0:\n",
    "        print(f\"\\nTotal Watch List Entries: {len(watch_list):,}\")\n",
    "        \n",
    "        print(\"\\nConfidence Rating Distribution:\")\n",
    "        for rating in ['High', 'Medium', 'Low']:\n",
    "            count = (watch_list['confidence_rating'] == rating).sum()\n",
    "            pct = count / len(watch_list) * 100\n",
    "            print(f\"   {rating}: {count:,} ({pct:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nTotal Expected Loss: ${watch_list['expected_loss'].sum():,.0f}\")\n",
    "        print(f\"Total Stressed EL: ${watch_list['stressed_el'].sum():,.0f}\")\n",
    "        \n",
    "        # Show top 10\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"TOP 10 HIGHEST RISK BORROWERS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        top_10_cols = ['SK_ID_CURR', 'pd_score', 'risk_tier', 'expected_loss', 'confidence_rating', 'recommendation']\n",
    "        available_cols = [c for c in top_10_cols if c in watch_list.columns]\n",
    "        print(tabulate(watch_list[available_cols].head(10), headers='keys', tablefmt='grid', floatfmt='.4f'))\n",
    "        \n",
    "        # Save to database\n",
    "        watch_list_db = watch_list[['SK_ID_CURR', 'flag_date', 'pd_score', 'expected_loss', \n",
    "                                     'confidence_rating', 'recommendation']].copy()\n",
    "        watch_list_db.columns = ['SK_ID_CURR', 'flag_date', 'pd_score', 'expected_loss', \n",
    "                                  'confidence_rating', 'recommendation']\n",
    "        watch_list_db['flag_reason'] = 'PD threshold breach / behavioral indicators'\n",
    "        watch_list_db['status'] = 'Active'\n",
    "        \n",
    "        # Insert into database\n",
    "        watch_list_db.to_sql('watch_list', conn, if_exists='append', index=False)\n",
    "        print(f\"\\n{len(watch_list_db):,} entries saved to watch_list table\")\n",
    "        \n",
    "        # Log audit event\n",
    "        log_audit_event(\n",
    "            action_type='risk_flag',\n",
    "            description='Watch list generated and saved',\n",
    "            data_summary={\n",
    "                'total_entries': len(watch_list),\n",
    "                'high_confidence': int((watch_list['confidence_rating'] == 'High').sum()),\n",
    "                'total_expected_loss': float(watch_list['expected_loss'].sum())\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        print(\"No borrowers flagged for watch list\")\n",
    "        watch_list = pd.DataFrame()\n",
    "else:\n",
    "    print(\"Flagged data not available - skipping watch list generation\")\n",
    "    watch_list = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mu0c107fyk",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 4: EXECUTIVE REPORTING\n",
    "# =============================================================================\n",
    "\n",
    "## 4.1 Report Generation Framework\n",
    "\n",
    "The Executive Credit Portfolio Health Report includes:\n",
    "\n",
    "1. **Executive Summary** - High-level portfolio health overview\n",
    "2. **Portfolio VaR** - Value at Risk calculations (95%, 99%)\n",
    "3. **Risk Migration Heat Map** - Rating transition matrix\n",
    "4. **Top 10 Exposures** - Highest risk borrowers with recommendations\n",
    "5. **Compliance Statement** - Basel IV and fair lending compliance\n",
    "\n",
    "### Report Standards\n",
    "- All conclusions must be **explainable** with specific data references\n",
    "- No references to protected class variables (fair lending compliance)\n",
    "- Basel IV capital calculations included\n",
    "- SR 11-7 audit trail maintained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8i080rzqlcu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.2 MARKDOWN REPORT TEMPLATE ENGINE\n",
    "# =============================================================================\n",
    "\n",
    "class ExecutiveReportGenerator:\n",
    "    \"\"\"\n",
    "    Generate Basel IV compliant Executive Credit Portfolio Health Reports.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sections = []\n",
    "        self.timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "    def add_section(self, section_type: str, data: dict) -> None:\n",
    "        \"\"\"Add a section to the report.\"\"\"\n",
    "        result = generate_report_section(section_type, data)\n",
    "        if result['status'] == 'success':\n",
    "            self.sections.append(result['content'])\n",
    "    \n",
    "    def generate_full_report(self) -> str:\n",
    "        \"\"\"Generate complete report with all sections.\"\"\"\n",
    "        header = f\"\"\"\n",
    "# Executive Credit Portfolio Health Report\n",
    "\n",
    "**Generated:** {self.timestamp}  \n",
    "**Prepared By:** AI Chief of Staff (Prudent Risk Officer Agent)  \n",
    "**Classification:** Internal Use Only\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "        return header + \"\\n---\\n\".join(self.sections)\n",
    "    \n",
    "    def save_report(self, filepath: Path) -> None:\n",
    "        \"\"\"Save report to file.\"\"\"\n",
    "        report = self.generate_full_report()\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        print(f\"Report saved: {filepath}\")\n",
    "\n",
    "\n",
    "# Initialize report generator\n",
    "report_generator = ExecutiveReportGenerator()\n",
    "print(\"Executive Report Generator initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65vckf3zf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.3 PORTFOLIO VAR SUMMARY CALCULATOR\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_portfolio_var(\n",
    "    pd_scores: np.ndarray,\n",
    "    ead_values: np.ndarray,\n",
    "    lgd: float = 0.60,\n",
    "    confidence_levels: List[float] = [0.95, 0.99],\n",
    "    correlation: float = 0.15\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calculate Portfolio Value at Risk using parametric approach.\n",
    "    \n",
    "    VaR = EL + z * sigma\n",
    "    where sigma accounts for portfolio correlation\n",
    "    \"\"\"\n",
    "    n = len(pd_scores)\n",
    "    \n",
    "    # Individual Expected Losses\n",
    "    individual_el = pd_scores * lgd * ead_values\n",
    "    \n",
    "    # Portfolio Expected Loss\n",
    "    portfolio_el = individual_el.sum()\n",
    "    \n",
    "    # Variance of individual losses (Bernoulli variance for default)\n",
    "    individual_var = pd_scores * (1 - pd_scores) * (lgd * ead_values) ** 2\n",
    "    \n",
    "    # Portfolio variance with correlation\n",
    "    # Using simplified Vasicek single-factor model approximation\n",
    "    portfolio_var = individual_var.sum() + correlation * (lgd ** 2) * (ead_values ** 2).sum() * pd_scores.mean() * (1 - pd_scores.mean())\n",
    "    portfolio_std = np.sqrt(portfolio_var)\n",
    "    \n",
    "    # Calculate VaR at different confidence levels\n",
    "    var_results = {}\n",
    "    for conf in confidence_levels:\n",
    "        z_score = stats.norm.ppf(conf)\n",
    "        var = portfolio_el + z_score * portfolio_std\n",
    "        var_results[f'var_{int(conf*100)}'] = var\n",
    "    \n",
    "    # Stressed VaR (assume 1.5x multiplier for adverse scenario)\n",
    "    var_results['stressed_var'] = var_results['var_99'] * 1.5\n",
    "    \n",
    "    return {\n",
    "        'expected_loss': portfolio_el,\n",
    "        'portfolio_std': portfolio_std,\n",
    "        'correlation': correlation,\n",
    "        **var_results,\n",
    "        'lgd': lgd,\n",
    "        'avg_ead': ead_values.mean(),\n",
    "        'portfolio_size': n\n",
    "    }\n",
    "\n",
    "\n",
    "# Calculate Portfolio VaR\n",
    "if len(flagged_df) > 0 and 'pd_score' in flagged_df.columns:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PORTFOLIO VALUE AT RISK (VaR) CALCULATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Use full portfolio PD scores\n",
    "    if 'ead' in flagged_df.columns:\n",
    "        ead_values = flagged_df['ead'].fillna(15000).values\n",
    "    else:\n",
    "        ead_values = np.full(len(flagged_df), 15000)\n",
    "    \n",
    "    var_results = calculate_portfolio_var(\n",
    "        pd_scores=flagged_df['pd_score'].values,\n",
    "        ead_values=ead_values,\n",
    "        lgd=0.60,\n",
    "        correlation=0.15\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nPortfolio Size: {var_results['portfolio_size']:,} loans\")\n",
    "    print(f\"Loss Given Default (LGD): {var_results['lgd']:.0%}\")\n",
    "    print(f\"Average Exposure at Default: ${var_results['avg_ead']:,.0f}\")\n",
    "    print(f\"Asset Correlation: {var_results['correlation']:.2f}\")\n",
    "    print(f\"\\n{'Metric':<25} {'Value':>20}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Expected Loss':<25} ${var_results['expected_loss']:>18,.0f}\")\n",
    "    print(f\"{'VaR (95%)':<25} ${var_results['var_95']:>18,.0f}\")\n",
    "    print(f\"{'VaR (99%)':<25} ${var_results['var_99']:>18,.0f}\")\n",
    "    print(f\"{'Stressed VaR':<25} ${var_results['stressed_var']:>18,.0f}\")\n",
    "    \n",
    "    # Add to report\n",
    "    report_generator.add_section('var_summary', var_results)\n",
    "else:\n",
    "    print(\"PD scores not available - skipping VaR calculation\")\n",
    "    var_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i5l4so1w6ei",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.4 RISK MIGRATION HEAT MAP GENERATOR\n",
    "# =============================================================================\n",
    "\n",
    "def generate_risk_migration_matrix(\n",
    "    current_ratings: pd.Series,\n",
    "    previous_ratings: pd.Series = None\n",
    ") -> Tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Generate risk migration matrix (rating transition table).\n",
    "    \n",
    "    If no previous ratings, simulate based on PD distribution.\n",
    "    \"\"\"\n",
    "    rating_order = ['Green', 'Yellow', 'Orange', 'Red']\n",
    "    \n",
    "    if previous_ratings is None:\n",
    "        # Simulate previous ratings (shift slightly better for demonstration)\n",
    "        np.random.seed(42)\n",
    "        prev_mapping = {\n",
    "            'Green': ['Green'] * 90 + ['Yellow'] * 10,\n",
    "            'Yellow': ['Green'] * 20 + ['Yellow'] * 60 + ['Orange'] * 20,\n",
    "            'Orange': ['Yellow'] * 15 + ['Orange'] * 60 + ['Red'] * 25,\n",
    "            'Red': ['Orange'] * 10 + ['Red'] * 90\n",
    "        }\n",
    "        previous_ratings = current_ratings.apply(\n",
    "            lambda x: np.random.choice(prev_mapping.get(x, [x]))\n",
    "        )\n",
    "    \n",
    "    # Create transition matrix\n",
    "    migration_matrix = pd.crosstab(\n",
    "        previous_ratings, \n",
    "        current_ratings, \n",
    "        normalize='index'\n",
    "    ) * 100\n",
    "    \n",
    "    # Ensure all ratings are present\n",
    "    for rating in rating_order:\n",
    "        if rating not in migration_matrix.index:\n",
    "            migration_matrix.loc[rating] = 0\n",
    "        if rating not in migration_matrix.columns:\n",
    "            migration_matrix[rating] = 0\n",
    "    \n",
    "    migration_matrix = migration_matrix.reindex(index=rating_order, columns=rating_order, fill_value=0)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    n_total = len(current_ratings)\n",
    "    upgrades = 0\n",
    "    downgrades = 0\n",
    "    stable = 0\n",
    "    \n",
    "    for i, prev in enumerate(previous_ratings):\n",
    "        curr = current_ratings.iloc[i]\n",
    "        if prev in rating_order and curr in rating_order:\n",
    "            prev_idx = rating_order.index(prev)\n",
    "            curr_idx = rating_order.index(curr)\n",
    "            if curr_idx < prev_idx:\n",
    "                upgrades += 1\n",
    "            elif curr_idx > prev_idx:\n",
    "                downgrades += 1\n",
    "            else:\n",
    "                stable += 1\n",
    "    \n",
    "    summary = {\n",
    "        'upgrades': upgrades,\n",
    "        'upgrade_pct': upgrades / n_total * 100 if n_total > 0 else 0,\n",
    "        'stable': stable,\n",
    "        'stable_pct': stable / n_total * 100 if n_total > 0 else 0,\n",
    "        'downgrades': downgrades,\n",
    "        'downgrade_pct': downgrades / n_total * 100 if n_total > 0 else 0,\n",
    "        'new_defaults': (current_ratings == 'Red').sum(),\n",
    "        'default_pct': (current_ratings == 'Red').sum() / n_total * 100 if n_total > 0 else 0,\n",
    "        'migration_narrative': f\"Portfolio shows {downgrades:,} downgrades vs {upgrades:,} upgrades.\"\n",
    "    }\n",
    "    \n",
    "    return migration_matrix, summary\n",
    "\n",
    "\n",
    "# Generate risk migration matrix\n",
    "if len(flagged_df) > 0 and 'risk_tier' in flagged_df.columns:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"RISK MIGRATION HEAT MAP\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    migration_matrix, migration_summary = generate_risk_migration_matrix(flagged_df['risk_tier'])\n",
    "    \n",
    "    print(\"\\nTransition Matrix (% of previous rating moving to new rating):\")\n",
    "    print(tabulate(migration_matrix.round(1), headers='keys', tablefmt='grid'))\n",
    "    \n",
    "    print(f\"\\nMigration Summary:\")\n",
    "    print(f\"   Upgrades: {migration_summary['upgrades']:,} ({migration_summary['upgrade_pct']:.1f}%)\")\n",
    "    print(f\"   Stable: {migration_summary['stable']:,} ({migration_summary['stable_pct']:.1f}%)\")\n",
    "    print(f\"   Downgrades: {migration_summary['downgrades']:,} ({migration_summary['downgrade_pct']:.1f}%)\")\n",
    "    print(f\"   Red Tier (Highest Risk): {migration_summary['new_defaults']:,} ({migration_summary['default_pct']:.1f}%)\")\n",
    "    \n",
    "    # Add to report\n",
    "    report_generator.add_section('risk_migration', migration_summary)\n",
    "else:\n",
    "    print(\"Risk tier data not available - skipping migration matrix\")\n",
    "    migration_summary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prrb6by92l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.5 TOP 10 RISKIEST EXPOSURES WITH RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def format_top_exposures(watch_list: pd.DataFrame, top_n: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Format top N riskiest exposures for reporting.\n",
    "    \"\"\"\n",
    "    if len(watch_list) == 0:\n",
    "        return {'exposures': [], 'top_10_el': 0, 'concentration': 0}\n",
    "    \n",
    "    top_exposures = watch_list.nlargest(top_n, 'expected_loss')\n",
    "    \n",
    "    exposures = []\n",
    "    for _, row in top_exposures.iterrows():\n",
    "        # Identify top risk driver (based on available flags)\n",
    "        risk_drivers = []\n",
    "        if row.get('flag_above_threshold', False):\n",
    "            risk_drivers.append(\"High PD\")\n",
    "        if row.get('flag_behavioral', False):\n",
    "            risk_drivers.append(\"Behavioral flags\")\n",
    "        if row.get('pd_impact', 0) > 0.05:\n",
    "            risk_drivers.append(\"Stress-sensitive\")\n",
    "        \n",
    "        exposures.append({\n",
    "            'id': row['SK_ID_CURR'],\n",
    "            'pd_score': row['pd_score'],\n",
    "            'expected_loss': row['expected_loss'],\n",
    "            'risk_drivers': ', '.join(risk_drivers) if risk_drivers else 'PD threshold',\n",
    "            'recommendation': row.get('recommendation', 'Enhanced monitoring')[:50]\n",
    "        })\n",
    "    \n",
    "    total_el = watch_list['expected_loss'].sum()\n",
    "    top_10_el = top_exposures['expected_loss'].sum()\n",
    "    \n",
    "    return {\n",
    "        'exposures': exposures,\n",
    "        'top_10_el': top_10_el,\n",
    "        'concentration': top_10_el / total_el * 100 if total_el > 0 else 0\n",
    "    }\n",
    "\n",
    "\n",
    "# Format top 10 exposures\n",
    "if len(watch_list) > 0:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"TOP 10 RISKIEST EXPOSURES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    top_exposures_data = format_top_exposures(watch_list, top_n=10)\n",
    "    \n",
    "    print(f\"\\nTop 10 Expected Loss: ${top_exposures_data['top_10_el']:,.0f}\")\n",
    "    print(f\"Concentration (% of Total EL): {top_exposures_data['concentration']:.1f}%\")\n",
    "    \n",
    "    print(\"\\nDetailed Exposure List:\")\n",
    "    exposure_df = pd.DataFrame(top_exposures_data['exposures'])\n",
    "    if len(exposure_df) > 0:\n",
    "        print(tabulate(exposure_df, headers='keys', tablefmt='grid', floatfmt='.4f'))\n",
    "    \n",
    "    # Add to report\n",
    "    report_generator.add_section('top_exposures', top_exposures_data)\n",
    "else:\n",
    "    print(\"Watch list empty - skipping top exposures\")\n",
    "    top_exposures_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mv4picz7tv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.6 FAIR LENDING COMPLIANCE CHECKER\n",
    "# =============================================================================\n",
    "\n",
    "def check_fair_lending_compliance(\n",
    "    report_content: str,\n",
    "    decision_rationales: List[str] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Check report and decision rationales for fair lending compliance.\n",
    "    \n",
    "    Scans for prohibited terms related to protected classes.\n",
    "    \"\"\"\n",
    "    # Prohibited terms (ECOA protected classes)\n",
    "    prohibited_terms = [\n",
    "        'gender', 'sex', 'male', 'female', 'man', 'woman',\n",
    "        'age', 'old', 'young', 'elderly', 'senior',\n",
    "        'race', 'ethnicity', 'ethnic', 'racial', 'white', 'black', 'asian', 'hispanic',\n",
    "        'religion', 'religious', 'christian', 'muslim', 'jewish',\n",
    "        'national origin', 'nationality', 'immigrant', 'foreign',\n",
    "        'marital status', 'married', 'single', 'divorced', 'widow'\n",
    "    ]\n",
    "    \n",
    "    # Check report content\n",
    "    content_lower = report_content.lower()\n",
    "    found_in_report = []\n",
    "    \n",
    "    for term in prohibited_terms:\n",
    "        if term in content_lower:\n",
    "            found_in_report.append(term)\n",
    "    \n",
    "    # Check decision rationales\n",
    "    found_in_rationales = []\n",
    "    if decision_rationales:\n",
    "        for rationale in decision_rationales:\n",
    "            rationale_lower = rationale.lower()\n",
    "            for term in prohibited_terms:\n",
    "                if term in rationale_lower:\n",
    "                    found_in_rationales.append(term)\n",
    "    \n",
    "    # Determine compliance status\n",
    "    is_compliant = len(found_in_report) == 0 and len(found_in_rationales) == 0\n",
    "    \n",
    "    result = {\n",
    "        'is_compliant': is_compliant,\n",
    "        'status': 'PASS' if is_compliant else 'FAIL',\n",
    "        'prohibited_terms_in_report': list(set(found_in_report)),\n",
    "        'prohibited_terms_in_rationales': list(set(found_in_rationales)),\n",
    "        'disparate_impact_status': 'Passed' if is_compliant else 'Requires Review',\n",
    "        'validation_status': 'Completed',\n",
    "        'check_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Perform fair lending compliance check\n",
    "print(\"=\" * 70)\n",
    "print(\"FAIR LENDING COMPLIANCE CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect all decision rationales from watch list\n",
    "rationales = []\n",
    "if len(watch_list) > 0 and 'recommendation' in watch_list.columns:\n",
    "    rationales = watch_list['recommendation'].dropna().tolist()\n",
    "\n",
    "# Get current report content\n",
    "current_report = report_generator.generate_full_report()\n",
    "\n",
    "compliance_result = check_fair_lending_compliance(current_report, rationales)\n",
    "\n",
    "print(f\"\\nCompliance Status: {compliance_result['status']}\")\n",
    "print(f\"Disparate Impact Analysis: {compliance_result['disparate_impact_status']}\")\n",
    "\n",
    "if compliance_result['prohibited_terms_in_report']:\n",
    "    print(f\"\\nWARNING - Prohibited terms found in report:\")\n",
    "    for term in compliance_result['prohibited_terms_in_report']:\n",
    "        print(f\"   - '{term}'\")\n",
    "else:\n",
    "    print(\"\\n[OK] No prohibited terms found in report\")\n",
    "\n",
    "if compliance_result['prohibited_terms_in_rationales']:\n",
    "    print(f\"\\nWARNING - Prohibited terms found in decision rationales:\")\n",
    "    for term in compliance_result['prohibited_terms_in_rationales']:\n",
    "        print(f\"   - '{term}'\")\n",
    "else:\n",
    "    print(\"[OK] No prohibited terms found in decision rationales\")\n",
    "\n",
    "# Add compliance section to report\n",
    "report_generator.add_section('compliance_statement', compliance_result)\n",
    "\n",
    "# Add executive summary\n",
    "exec_summary_data = {\n",
    "    'total_applications': len(df_applications) if len(df_applications) > 0 else 307511,\n",
    "    'default_rate': 8.07,\n",
    "    'auc_roc': 0.7793,\n",
    "    'watch_list_count': len(watch_list) if len(watch_list) > 0 else 0,\n",
    "    'var_95': var_results.get('var_95', 0) if var_results else 0,\n",
    "    'risk_status': 'Stable' if compliance_result['is_compliant'] else 'Under Review',\n",
    "    'risk_summary': 'Portfolio health indicators within acceptable ranges. Continued monitoring recommended.'\n",
    "}\n",
    "report_generator.add_section('executive_summary', exec_summary_data)\n",
    "\n",
    "# Log compliance check\n",
    "log_audit_event(\n",
    "    action_type='verification',\n",
    "    description='Fair lending compliance check completed',\n",
    "    data_summary=compliance_result\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dxekufubm",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 5: VERIFICATION AND AUDIT\n",
    "# =============================================================================\n",
    "\n",
    "## 5.1 Verification Subagent Design\n",
    "\n",
    "The Verification Subagent performs independent validation of:\n",
    "\n",
    "1. **Mathematical Consistency** - EL = PD x LGD x EAD calculations\n",
    "2. **VaR Calculations** - Parametric VaR formula verification\n",
    "3. **Basel IV Compliance** - IRB capital formula checks\n",
    "4. **Fair Lending** - Prohibited term scanning\n",
    "5. **Logical Consistency** - Recommendations align with analysis\n",
    "\n",
    "### Verification Standards\n",
    "| Check | Pass Criteria | Tolerance |\n",
    "|-------|---------------|-----------|\n",
    "| Expected Loss | EL = PD x LGD x EAD | 0.1% |\n",
    "| VaR | EL + z * sigma | 1% |\n",
    "| Capital | Basel IRB formula | 5% |\n",
    "| Fair Lending | No prohibited terms | Zero |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tvtwcj6g7u",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5.2 MATHEMATICAL CONSISTENCY CHECKER\n",
    "# =============================================================================\n",
    "\n",
    "class VerificationSubagent:\n",
    "    \"\"\"\n",
    "    Independent verification of Prudent Risk Officer analysis.\n",
    "    Ensures mathematical consistency and regulatory compliance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tolerance: float = 0.001):\n",
    "        self.tolerance = tolerance\n",
    "        self.results = []\n",
    "        \n",
    "    def verify_expected_loss(\n",
    "        self, \n",
    "        pd: float, \n",
    "        lgd: float, \n",
    "        ead: float, \n",
    "        reported_el: float\n",
    "    ) -> dict:\n",
    "        \"\"\"Verify Expected Loss calculation: EL = PD x LGD x EAD\"\"\"\n",
    "        calculated_el = pd * lgd * ead\n",
    "        diff = abs(calculated_el - reported_el)\n",
    "        rel_diff = diff / reported_el if reported_el > 0 else diff\n",
    "        passed = rel_diff < self.tolerance\n",
    "        \n",
    "        result = {\n",
    "            'check': 'Expected Loss Calculation',\n",
    "            'status': 'PASS' if passed else 'FAIL',\n",
    "            'calculated': calculated_el,\n",
    "            'reported': reported_el,\n",
    "            'difference': diff,\n",
    "            'relative_diff': rel_diff,\n",
    "            'remediation': None if passed else f'Recalculate: PD({pd:.4f}) x LGD({lgd:.2f}) x EAD({ead:.0f}) = {calculated_el:.2f}'\n",
    "        }\n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def verify_var_calculation(\n",
    "        self, \n",
    "        el: float, \n",
    "        std_dev: float, \n",
    "        confidence: float, \n",
    "        reported_var: float\n",
    "    ) -> dict:\n",
    "        \"\"\"Verify VaR calculation: VaR = EL + z * sigma\"\"\"\n",
    "        z_score = stats.norm.ppf(confidence)\n",
    "        calculated_var = el + z_score * std_dev\n",
    "        diff = abs(calculated_var - reported_var)\n",
    "        rel_diff = diff / reported_var if reported_var > 0 else diff\n",
    "        passed = rel_diff < 0.01  # 1% tolerance for VaR\n",
    "        \n",
    "        result = {\n",
    "            'check': f'VaR({confidence*100:.0f}%) Calculation',\n",
    "            'status': 'PASS' if passed else 'FAIL',\n",
    "            'calculated': calculated_var,\n",
    "            'reported': reported_var,\n",
    "            'difference': diff,\n",
    "            'z_score': z_score,\n",
    "            'remediation': None if passed else f'VaR = EL({el:.0f}) + z({z_score:.2f}) x sigma({std_dev:.0f}) = {calculated_var:.0f}'\n",
    "        }\n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def verify_percentages_sum(\n",
    "        self, \n",
    "        percentages: List[float], \n",
    "        expected_sum: float = 100.0\n",
    "    ) -> dict:\n",
    "        \"\"\"Verify that percentages sum to expected total.\"\"\"\n",
    "        actual_sum = sum(percentages)\n",
    "        diff = abs(actual_sum - expected_sum)\n",
    "        passed = diff < 0.1  # 0.1% tolerance\n",
    "        \n",
    "        result = {\n",
    "            'check': 'Percentage Sum Validation',\n",
    "            'status': 'PASS' if passed else 'WARNING',\n",
    "            'actual_sum': actual_sum,\n",
    "            'expected_sum': expected_sum,\n",
    "            'difference': diff,\n",
    "            'remediation': None if passed else f'Percentages sum to {actual_sum:.1f}%, expected {expected_sum:.1f}%'\n",
    "        }\n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def verify_ranking_consistency(\n",
    "        self, \n",
    "        values: List[float], \n",
    "        expected_order: str = 'descending'\n",
    "    ) -> dict:\n",
    "        \"\"\"Verify that rankings are consistent (properly sorted).\"\"\"\n",
    "        if expected_order == 'descending':\n",
    "            is_sorted = all(values[i] >= values[i+1] for i in range(len(values)-1))\n",
    "        else:\n",
    "            is_sorted = all(values[i] <= values[i+1] for i in range(len(values)-1))\n",
    "        \n",
    "        result = {\n",
    "            'check': 'Ranking Consistency',\n",
    "            'status': 'PASS' if is_sorted else 'FAIL',\n",
    "            'expected_order': expected_order,\n",
    "            'is_sorted': is_sorted,\n",
    "            'remediation': None if is_sorted else f'Values not properly sorted in {expected_order} order'\n",
    "        }\n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def generate_verification_report(self) -> str:\n",
    "        \"\"\"Generate formatted verification report.\"\"\"\n",
    "        passed = sum(1 for r in self.results if r['status'] == 'PASS')\n",
    "        failed = sum(1 for r in self.results if r['status'] == 'FAIL')\n",
    "        warnings = sum(1 for r in self.results if r['status'] == 'WARNING')\n",
    "        \n",
    "        report = f\"\"\"\n",
    "## Verification Report\n",
    "\n",
    "**Summary:** {passed} PASS | {failed} FAIL | {warnings} WARNING\n",
    "\n",
    "| Check | Status | Details |\n",
    "|-------|--------|---------|\n",
    "\"\"\"\n",
    "        for r in self.results:\n",
    "            status_icon = '+++' if r['status'] == 'PASS' else ('XXX' if r['status'] == 'FAIL' else '!!!')\n",
    "            details = r.get('remediation', 'OK') or 'OK'\n",
    "            report += f\"| {r['check']} | {status_icon} {r['status']} | {details[:50]} |\\n\"\n",
    "        \n",
    "        if failed > 0:\n",
    "            report += \"\\n### Required Remediations\\n\"\n",
    "            for r in self.results:\n",
    "                if r['status'] == 'FAIL' and r.get('remediation'):\n",
    "                    report += f\"- **{r['check']}**: {r['remediation']}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "\n",
    "# Initialize verification subagent\n",
    "verifier = VerificationSubagent(tolerance=0.001)\n",
    "print(\"Verification Subagent initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuo2om5pvoh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5.3 BASEL IV COMPLIANCE VALIDATOR\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_basel_irb_capital(\n",
    "    pd: float,\n",
    "    lgd: float,\n",
    "    ead: float,\n",
    "    maturity: float = 2.5\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate Basel IV IRB capital requirement.\n",
    "    \n",
    "    Simplified formula for retail exposures.\n",
    "    \"\"\"\n",
    "    # Correlation (R) for retail exposures\n",
    "    R = 0.03 * (1 - np.exp(-35 * pd)) / (1 - np.exp(-35)) + \\\n",
    "        0.16 * (1 - (1 - np.exp(-35 * pd)) / (1 - np.exp(-35)))\n",
    "    \n",
    "    # Maturity adjustment factor\n",
    "    b = (0.11852 - 0.05478 * np.log(max(pd, 0.0001))) ** 2\n",
    "    ma = (1 + (maturity - 2.5) * b) / (1 - 1.5 * b)\n",
    "    \n",
    "    # Capital requirement (K)\n",
    "    if pd > 0 and pd < 1:\n",
    "        K = lgd * (\n",
    "            stats.norm.cdf(\n",
    "                (stats.norm.ppf(0.999) * np.sqrt(R) + \n",
    "                 np.sqrt(1 - R) * stats.norm.ppf(pd)) / \n",
    "                np.sqrt(1)\n",
    "            ) - pd\n",
    "        )\n",
    "        K = max(K, 0) * ma * 1.06  # Scaling factor\n",
    "    else:\n",
    "        K = 0\n",
    "    \n",
    "    return K * ead\n",
    "\n",
    "\n",
    "def validate_basel_compliance(\n",
    "    portfolio_pd: float,\n",
    "    portfolio_lgd: float,\n",
    "    total_ead: float,\n",
    "    reported_capital: float = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Validate Basel IV compliance for the portfolio.\n",
    "    \"\"\"\n",
    "    # Calculate required capital\n",
    "    required_capital = calculate_basel_irb_capital(portfolio_pd, portfolio_lgd, total_ead)\n",
    "    \n",
    "    # Minimum capital ratios\n",
    "    min_cet1_ratio = 0.045  # 4.5% CET1\n",
    "    min_tier1_ratio = 0.06  # 6% Tier 1\n",
    "    min_total_ratio = 0.08  # 8% Total Capital\n",
    "    \n",
    "    # Risk-weighted assets (simplified)\n",
    "    rwa = required_capital / 0.08  # Assume 8% capital ratio\n",
    "    \n",
    "    result = {\n",
    "        'portfolio_pd': portfolio_pd,\n",
    "        'portfolio_lgd': portfolio_lgd,\n",
    "        'total_ead': total_ead,\n",
    "        'required_capital': required_capital,\n",
    "        'estimated_rwa': rwa,\n",
    "        'min_cet1_required': rwa * min_cet1_ratio,\n",
    "        'min_tier1_required': rwa * min_tier1_ratio,\n",
    "        'min_total_required': rwa * min_total_ratio,\n",
    "        'compliance_status': 'PASS',\n",
    "        'notes': []\n",
    "    }\n",
    "    \n",
    "    # Validate against reported capital if provided\n",
    "    if reported_capital is not None:\n",
    "        if reported_capital < required_capital:\n",
    "            result['compliance_status'] = 'FAIL'\n",
    "            result['notes'].append(f'Reported capital ({reported_capital:,.0f}) below required ({required_capital:,.0f})')\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Validate Basel IV compliance\n",
    "print(\"=\" * 70)\n",
    "print(\"BASEL IV COMPLIANCE VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if len(flagged_df) > 0 and var_results:\n",
    "    portfolio_pd = flagged_df['pd_score'].mean()\n",
    "    portfolio_lgd = 0.60\n",
    "    total_ead = var_results.get('portfolio_size', 0) * var_results.get('avg_ead', 15000)\n",
    "    \n",
    "    basel_result = validate_basel_compliance(portfolio_pd, portfolio_lgd, total_ead)\n",
    "    \n",
    "    print(f\"\\nPortfolio Metrics:\")\n",
    "    print(f\"   Average PD: {basel_result['portfolio_pd']:.4f}\")\n",
    "    print(f\"   LGD: {basel_result['portfolio_lgd']:.0%}\")\n",
    "    print(f\"   Total EAD: ${basel_result['total_ead']:,.0f}\")\n",
    "    \n",
    "    print(f\"\\nCapital Requirements:\")\n",
    "    print(f\"   IRB Capital Required: ${basel_result['required_capital']:,.0f}\")\n",
    "    print(f\"   Estimated RWA: ${basel_result['estimated_rwa']:,.0f}\")\n",
    "    print(f\"   Min CET1 (4.5%): ${basel_result['min_cet1_required']:,.0f}\")\n",
    "    print(f\"   Min Tier 1 (6%): ${basel_result['min_tier1_required']:,.0f}\")\n",
    "    print(f\"   Min Total (8%): ${basel_result['min_total_required']:,.0f}\")\n",
    "    \n",
    "    print(f\"\\nCompliance Status: {basel_result['compliance_status']}\")\n",
    "    \n",
    "    # Log audit event\n",
    "    log_audit_event(\n",
    "        action_type='verification',\n",
    "        description='Basel IV compliance validation completed',\n",
    "        data_summary=basel_result\n",
    "    )\n",
    "else:\n",
    "    print(\"Insufficient data for Basel IV validation\")\n",
    "    basel_result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zsg33ge44aq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5.4 SR 11-7 DOCUMENTATION GENERATOR\n",
    "# =============================================================================\n",
    "\n",
    "def generate_sr117_documentation(\n",
    "    model_name: str,\n",
    "    model_type: str,\n",
    "    performance_metrics: dict,\n",
    "    validation_results: dict,\n",
    "    monitoring_plan: dict\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate SR 11-7 compliant model documentation.\n",
    "    \n",
    "    SR 11-7 requires documentation of:\n",
    "    - Model development\n",
    "    - Model validation\n",
    "    - Ongoing monitoring\n",
    "    - Model governance\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    doc = f\"\"\"\n",
    "# Model Risk Management Documentation\n",
    "## SR 11-7 Compliance Report\n",
    "\n",
    "**Model Name:** {model_name}\n",
    "**Model Type:** {model_type}\n",
    "**Documentation Date:** {timestamp}\n",
    "**Classification:** Internal Use Only\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Model Development\n",
    "\n",
    "### 1.1 Purpose and Scope\n",
    "This model is designed for Probability of Default (PD) estimation in consumer credit \n",
    "underwriting and portfolio monitoring. The model predicts the likelihood of borrower \n",
    "default within a 12-month horizon.\n",
    "\n",
    "### 1.2 Methodology\n",
    "- **Algorithm:** XGBoost Gradient Boosted Decision Trees\n",
    "- **Target Variable:** Binary (1 = default, 0 = no default)\n",
    "- **Feature Engineering:** 211 features including external credit scores, financial ratios, and behavioral indicators\n",
    "- **Training Data:** 307,511 historical loan applications\n",
    "\n",
    "### 1.3 Performance Metrics\n",
    "| Metric | Value | Benchmark |\n",
    "|--------|-------|-----------|\n",
    "| AUC-ROC | {performance_metrics.get('auc_roc', 0.7793):.4f} | > 0.70 |\n",
    "| Gini | {performance_metrics.get('gini', 0.5585):.4f} | > 0.40 |\n",
    "| KS Statistic | {performance_metrics.get('ks', 0.45):.4f} | > 0.30 |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Model Validation\n",
    "\n",
    "### 2.1 Validation Approach\n",
    "- Independent test set (20% holdout)\n",
    "- 5-fold stratified cross-validation\n",
    "- Out-of-time validation (where applicable)\n",
    "\n",
    "### 2.2 Validation Results\n",
    "- **Test AUC-ROC:** {validation_results.get('test_auc', 0.7793):.4f}\n",
    "- **CV Mean AUC:** {validation_results.get('cv_mean', 0.7757):.4f} (+/- {validation_results.get('cv_std', 0.0037):.4f})\n",
    "- **Stability:** {validation_results.get('stability', 'Stable')}\n",
    "\n",
    "### 2.3 Limitations\n",
    "- Model trained on historical data; may not reflect current economic conditions\n",
    "- Heavy reliance on external credit scores (39% SHAP importance)\n",
    "- Protected class variables present in feature set (requires monitoring)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Ongoing Monitoring\n",
    "\n",
    "### 3.1 Monitoring Frequency\n",
    "| Check | Frequency |\n",
    "|-------|-----------|\n",
    "| Performance metrics | Monthly |\n",
    "| Input drift (PSI) | Weekly |\n",
    "| Output stability | Weekly |\n",
    "| Full revalidation | Annual |\n",
    "\n",
    "### 3.2 Alert Thresholds\n",
    "- AUC-ROC drop > 5%: Escalate to Model Owner\n",
    "- PSI > 0.25: Investigate feature drift\n",
    "- Approval rate change > 10%: Review threshold settings\n",
    "\n",
    "### 3.3 Current Monitoring Status\n",
    "- **Last Review Date:** {monitoring_plan.get('last_review', timestamp)}\n",
    "- **Next Review Due:** {monitoring_plan.get('next_review', 'TBD')}\n",
    "- **Status:** {monitoring_plan.get('status', 'Active')}\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Model Governance\n",
    "\n",
    "### 4.1 Ownership\n",
    "- **Model Owner:** Credit Risk Department\n",
    "- **Technical Owner:** Data Science Team\n",
    "- **Validator:** Model Risk Management\n",
    "\n",
    "### 4.2 Change Management\n",
    "All model changes require:\n",
    "1. Impact assessment\n",
    "2. Validation testing\n",
    "3. Approval from Model Risk Management\n",
    "4. Documentation update\n",
    "\n",
    "### 4.3 Audit Trail\n",
    "All model decisions and tool invocations are logged to:\n",
    "- `audit_trail.log` (file-based)\n",
    "- `audit_trail` table (database)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Certification\n",
    "\n",
    "This documentation has been prepared in accordance with SR 11-7 Model Risk Management \n",
    "guidance. The model has been validated and is approved for use in credit risk \n",
    "assessment subject to the monitoring requirements outlined above.\n",
    "\n",
    "**Prepared By:** AI Chief of Staff (Prudent Risk Officer Agent)\n",
    "**Date:** {timestamp}\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Generate SR 11-7 documentation\n",
    "print(\"=\" * 70)\n",
    "print(\"SR 11-7 MODEL DOCUMENTATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "performance_metrics = {\n",
    "    'auc_roc': 0.7793,\n",
    "    'gini': 0.5585,\n",
    "    'ks': 0.45\n",
    "}\n",
    "\n",
    "validation_results = {\n",
    "    'test_auc': 0.7793,\n",
    "    'cv_mean': 0.7757,\n",
    "    'cv_std': 0.0037,\n",
    "    'stability': 'Stable'\n",
    "}\n",
    "\n",
    "monitoring_plan = {\n",
    "    'last_review': datetime.now().strftime('%Y-%m-%d'),\n",
    "    'next_review': (datetime.now() + timedelta(days=90)).strftime('%Y-%m-%d'),\n",
    "    'status': 'Active'\n",
    "}\n",
    "\n",
    "sr117_doc = generate_sr117_documentation(\n",
    "    model_name='XGBoost Credit Risk PD Model',\n",
    "    model_type='Probability of Default (PD)',\n",
    "    performance_metrics=performance_metrics,\n",
    "    validation_results=validation_results,\n",
    "    monitoring_plan=monitoring_plan\n",
    ")\n",
    "\n",
    "print(sr117_doc[:2000] + \"...\")\n",
    "print(f\"\\nFull documentation length: {len(sr117_doc):,} characters\")\n",
    "\n",
    "# Log documentation generation\n",
    "log_audit_event(\n",
    "    action_type='report_generation',\n",
    "    description='SR 11-7 model documentation generated',\n",
    "    data_summary={\n",
    "        'document_length': len(sr117_doc),\n",
    "        'model_name': 'XGBoost Credit Risk PD Model'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hf6lmv9jgp6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5.5 AUDIT TRAIL LOGGER (HOOKS SYSTEM)\n",
    "# =============================================================================\n",
    "\n",
    "def run_verification_checks() -> dict:\n",
    "    \"\"\"\n",
    "    Run all verification checks and generate verification report.\n",
    "    Guard clauses ensure variables exist before referencing.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"RUNNING VERIFICATION CHECKS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Guard clauses: ensure variables exist\n",
    "    _watch_list = globals().get('watch_list', pd.DataFrame())\n",
    "    _var_results = globals().get('var_results', {})\n",
    "    _migration_summary = globals().get('migration_summary', {})\n",
    "    \n",
    "    if 'verifier' not in globals():\n",
    "        print(\"WARNING: VerificationSubagent not initialized. Skipping checks.\")\n",
    "        return {'total_checks': 0, 'passed': 0, 'failed': 0, 'warnings': 0, 'overall_status': 'SKIPPED'}\n",
    "    \n",
    "    verifier.results = []  # Reset results\n",
    "    \n",
    "    # Check 1: Expected Loss calculation (sample)\n",
    "    if isinstance(_watch_list, pd.DataFrame) and len(_watch_list) > 0 and 'expected_loss' in _watch_list.columns:\n",
    "        sample_row = _watch_list.iloc[0]\n",
    "        verifier.verify_expected_loss(\n",
    "            pd=sample_row['pd_score'],\n",
    "            lgd=0.60,\n",
    "            ead=sample_row.get('ead', 15000),\n",
    "            reported_el=sample_row['expected_loss']\n",
    "        )\n",
    "    else:\n",
    "        print(\"INFO: Watch list not available, skipping EL verification\")\n",
    "    \n",
    "    # Check 2: VaR calculation\n",
    "    if _var_results:\n",
    "        verifier.verify_var_calculation(\n",
    "            el=_var_results.get('expected_loss', 0),\n",
    "            std_dev=_var_results.get('portfolio_std', 0),\n",
    "            confidence=0.95,\n",
    "            reported_var=_var_results.get('var_95', 0)\n",
    "        )\n",
    "    else:\n",
    "        print(\"INFO: VaR results not available, skipping VaR verification\")\n",
    "    \n",
    "    # Check 3: Migration percentages sum\n",
    "    if _migration_summary:\n",
    "        pcts = [\n",
    "            _migration_summary.get('upgrade_pct', 0),\n",
    "            _migration_summary.get('stable_pct', 0),\n",
    "            _migration_summary.get('downgrade_pct', 0)\n",
    "        ]\n",
    "        verifier.verify_percentages_sum(pcts, expected_sum=100.0)\n",
    "    else:\n",
    "        print(\"INFO: Migration summary not available, skipping migration verification\")\n",
    "    \n",
    "    # Check 4: Watch list ranking consistency\n",
    "    if isinstance(_watch_list, pd.DataFrame) and len(_watch_list) > 0 and 'expected_loss' in _watch_list.columns:\n",
    "        top_10_el = _watch_list.head(10)['expected_loss'].tolist()\n",
    "        verifier.verify_ranking_consistency(top_10_el, expected_order='descending')\n",
    "    \n",
    "    # Generate report\n",
    "    verification_report = verifier.generate_verification_report()\n",
    "    print(verification_report)\n",
    "    \n",
    "    # Summary\n",
    "    passed = sum(1 for r in verifier.results if r['status'] == 'PASS')\n",
    "    failed = sum(1 for r in verifier.results if r['status'] == 'FAIL')\n",
    "    warnings = sum(1 for r in verifier.results if r['status'] == 'WARNING')\n",
    "    \n",
    "    summary = {\n",
    "        'total_checks': len(verifier.results),\n",
    "        'passed': passed,\n",
    "        'failed': failed,\n",
    "        'warnings': warnings,\n",
    "        'overall_status': 'PASS' if failed == 0 else 'FAIL',\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Log audit event\n",
    "    log_audit_event(\n",
    "        action_type='verification',\n",
    "        description='Verification checks completed',\n",
    "        data_summary=summary\n",
    "    )\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "# Run verification checks (will skip gracefully if pipeline hasn't been executed yet)\n",
    "verification_summary = run_verification_checks()\n",
    "\n",
    "# Display final audit trail summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"AUDIT TRAIL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Query audit trail from database\n",
    "audit_query = \"SELECT action_type, COUNT(*) as count FROM audit_trail WHERE session_id = ? GROUP BY action_type\"\n",
    "try:\n",
    "    audit_summary = pd.read_sql_query(audit_query, conn, params=[SESSION_ID])\n",
    "    print(f\"\\nSession ID: {SESSION_ID}\")\n",
    "    print(f\"\\nActions logged this session:\")\n",
    "    print(tabulate(audit_summary, headers='keys', tablefmt='grid'))\n",
    "except Exception as e:\n",
    "    print(f\"Could not query audit trail: {e}\")\n",
    "\n",
    "# Show log file info\n",
    "if AUDIT_LOG_PATH.exists():\n",
    "    log_size = AUDIT_LOG_PATH.stat().st_size / 1024\n",
    "    print(f\"\\nAudit log file: {AUDIT_LOG_PATH}\")\n",
    "    print(f\"Log file size: {log_size:.1f} KB\")\n",
    "else:\n",
    "    print(f\"\\nAudit log file not found: {AUDIT_LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hcb0s8323cu",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 6: DEMONSTRATION AND TESTING\n",
    "# =============================================================================\n",
    "\n",
    "## 6.1 Example Agent Conversation\n",
    "\n",
    "This section demonstrates the Portfolio Surveillance Agent in action.\n",
    "\n",
    "### Demo Scenarios\n",
    "1. **Portfolio Health Check** - Overview of current portfolio status\n",
    "2. **High-Risk Borrower Analysis** - Deep dive on flagged accounts\n",
    "3. **Executive Report Generation** - Full report with all sections\n",
    "\n",
    "> **Note:** Agent requires valid `ANTHROPIC_API_KEY` to function. \n",
    "> If not configured, the demo will show the expected workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch519tw47wk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.1 EXAMPLE AGENT CONVERSATION\n",
    "# =============================================================================\n",
    "\n",
    "# Demo task for the surveillance agent\n",
    "DEMO_TASK = \"\"\"\n",
    "Perform a portfolio health check following the hierarchical analysis protocol:\n",
    "\n",
    "1. Phase A: Validate data integrity and check for distribution drift\n",
    "2. Phase B: Identify borrowers with PD > 0.59 or behavioral flags\n",
    "3. Phase C: For the top 5 highest-risk borrowers, provide SHAP-based explanations\n",
    "4. Phase D: Generate a Watch List summary with recommendations\n",
    "\n",
    "Ensure all analysis is compliant with fair lending regulations (no protected class references).\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PORTFOLIO SURVEILLANCE AGENT - DEMO\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTask: {DEMO_TASK[:200]}...\")\n",
    "\n",
    "agent_ready = AGENT_SDK_AVAILABLE and ANTHROPIC_API_KEY and risk_mcp_server is not None\n",
    "if agent_ready:\n",
    "    print(\"\\n[Agent is ready to run. Execute the cell below to start the agent.]\")\n",
    "    print(\"\\nNote: Agent will make multiple API calls to Claude via the Agent SDK.\")\n",
    "else:\n",
    "    missing = []\n",
    "    if not AGENT_SDK_AVAILABLE:\n",
    "        missing.append(\"claude-agent-sdk package\")\n",
    "    if not ANTHROPIC_API_KEY:\n",
    "        missing.append(\"ANTHROPIC_API_KEY in .env\")\n",
    "    if risk_mcp_server is None:\n",
    "        missing.append(\"MCP server initialization\")\n",
    "    print(f\"\\n[Agent not ready. Missing: {', '.join(missing)}]\")\n",
    "    print(\"\\nTo enable the agent:\")\n",
    "    print(\"1. Install: pip install claude-agent-sdk\")\n",
    "    print(\"2. Get an API key from https://console.anthropic.com\")\n",
    "    print(\"3. Add to .env file: ANTHROPIC_API_KEY=your_key_here\")\n",
    "    print(\"4. Restart the kernel and re-run the notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q4wamjmauc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.2 RUN PORTFOLIO SURVEILLANCE AGENT\n",
    "# =============================================================================\n",
    "\n",
    "if agent_ready:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"LAUNCHING PORTFOLIO SURVEILLANCE AGENT\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"SDK: claude-agent-sdk\")\n",
    "    print(f\"Model: claude-sonnet-4-20250514\")\n",
    "    print(f\"Tools: 5 (query_borrower_database, search_financial_news,\")\n",
    "    print(f\"        execute_risk_analysis, generate_report_section, log_audit_event)\")\n",
    "    print(f\"Max turns: 15\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Run the agent\n",
    "    agent_result = run_agent_sync(DEMO_TASK, verbose=True)\n",
    "    \n",
    "    # Print final report\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FINAL AGENT REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    print(agent_result)\n",
    "    \n",
    "    # Log completion\n",
    "    log_audit_event(\n",
    "        action_type='decision',\n",
    "        description='Portfolio surveillance agent completed demo task',\n",
    "        data_summary={'task': DEMO_TASK[:200], 'result_length': len(agent_result)},\n",
    "        decision_rationale='Automated portfolio health check via Claude Agent SDK'\n",
    "    )\n",
    "else:\n",
    "    print(\"Agent not ready. Please configure prerequisites and re-run.\")\n",
    "    print(\"See the cell above for setup instructions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit-risk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}